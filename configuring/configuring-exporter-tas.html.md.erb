---
title: Configuring Healthwatch Exporter for TAS for VMs
owner: Healthwatch
---

<strong><%= modified_date %></strong>

This topic describes how to manually configure and deploy the Healthwatch Exporter for VMware
Tanzu Application Service for VMs (TAS for VMs) tile.

To install, configure, and deploy Healthwatch Exporter for TAS for VMs through an automated
pipeline, see [Installing, Configuring, and Deploying a Tile Through an Automated Pipeline]
(../installing/automated-pipeline.html).


## <a id='overview'></a> Overview of Configuring and Deploying Healthwatch Exporter for TAS for VMs

When installed on a foundation you want to monitor, Healthwatch Exporter for TAS for VMs deploys
metric exporter VMs to generate each type of metric related to the health of your TAS for VMs
deployment. Healthwatch Exporter for TAS for VMs sends metrics through the Loggregator Firehose
to a Prometheus exposition endpoint on the associated metric exporter VMs. The Prometheus instance
that exists within your metrics monitoring system then scrapes the exposition endpoints on
the metric exporter VMs and imports those metrics into your monitoring system. For more information
about the architecture of the Healthwatch Exporter for TAS for VMs tile, see [Healthwatch Exporter
for TAS for VMs](../architecture.html#healthwatch-tas) in _Reference Architecture_.

After installing Healthwatch Exporter for TAS for VMs, you configure the metric exporter VMs
deployed by Healthwatch Exporter for TAS for VMs through the tile UI. You can also configure
errands and system logging, as well as scale VM instances up or down and configure load balancers
for multiple VM instances.

To configure and deploy the Healthwatch Exporter for TAS for VMs tile:

<p class='note'><strong>Note:</strong> If you want to quickly deploy the Healthwatch Exporter
  for TAS for VMs tile to ensure that it deploys successfully before you fully configure it,
  you only need to configure the <strong>Assign AZ and Networks</strong> and <strong>BOSH Health
  Exporter Configuration</strong> panes.</p>

1. Ensure that you meet the prerequisite for configuring and deploying Healthwatch Exporter
for TAS for VMs. For more information, see [Prerequisite](#prerequisite) below.

1. Navigate to the Healthwatch Exporter for TAS for VMs tile in the Ops Manager Installation
Dashboard. For more information, see [Navigate to the Healthwatch Exporter for TAS for VMs
Tile](#open-ui) below.

1. Assign jobs to your Availability Zones (AZs) and networks. For more information, see [Assign
AZs and Networks](#az) below.

1. (Optional) Configure the **TAS Exporter Configuration** pane. For more information, see
[(Optional) Configure TAS for VMs Metric Exporter VMs](#exporter-config) below.

1. Configure the **BOSH Health Exporter Configuration** pane. For more information, see [Configure
the BOSH Health Metric Exporter VM](#bosh-exporter) below.

1. (Optional) Configure the **Bosh Deployments Exporter Configuration** pane. For more information,
see [(Optional) Configure the BOSH Deployment Metric Exporter VM](#bosh-deployments) below.

1. (Optional) Configure the **Errands** pane. For more information, see [(Optional) Configure
Errands](#errands) below.

1. (Optional) Configure the **Syslog** pane. For more information, see [(Optional) Configure
Syslog](#syslog) below.

1. (Optional) Configure the **Resource Config** pane. For more information, see [(Optional)
Configure Resources](#resource-config) below.

1. Deploy the Healthwatch Exporter for TAS for VMs tile through the Ops Manager Installation
Dashboard. For more information, see [Deploy Healthwatch Exporter for TAS for VMs](#apply-changes)
below.

1. Once you have finished installing, configuring, and deploying Healthwatch Exporter for TAS
for VMs, configure a scrape job for Healthwatch Exporter for TAS for VMs in the Prometheus
VM that exists within your monitoring system. For more information, see [Configure a Scrape
Job for Healthwatch Exporter for TAS for VMs](#scrape-job-config) below.
  <p class='note'><strong>Note:</strong> You only need to configure a scrape job for installations
  of Healthwatch Exporter for TAS for VMs that are not on the same foundation as your Healthwatch
  tile. The Prometheus instance in the Healthwatch tile automatically discovers and scrapes
  Healthwatch Exporter tiles that are installed on the same foundation as the Healthwatch tile.</p>


## <a id='prerequisite'></a> Prerequisite

Before you deploy Healthwatch Exporter for TAS for VMs, you must have a metrics monitoring
system that collects metrics using a Prometheus instance. This monitoring system can be one
of the following:

* The Healthwatch tile installed on either the same Ops Manager foundation as Healthwatch Exporter
for TAS for VMs or a different Ops Manager foundation.

* A service or database located outside your Ops Manager foundation, such as an external time
series database (TSDB) or an installation of the Healthwatch tile on the Tanzu Kubernetes Grid
Integrated Edition (TKGI) Control Plane.


## <a id='open-ui'></a> Navigate to the Healthwatch Exporter for TAS for VMs Tile

To navigate to the Healthwatch Exporter for TAS for VMs tile:

1. Navigate to the Ops Manager Installation Dashboard.

1. Click the **Healthwatch Exporter for Tanzu Application Service** tile.


## <a id='az'></a> Assign AZs and Networks

In the **Assign AZ and Networks** pane, you assign jobs to your AZs and networks.

To configure the **Assign AZ and Networks** pane:

1. Select **Assign AZs and Networks**.

1. Under **Place singleton jobs in**, select the first AZ. Ops Manager runs any job with a
single instance in this AZ.

1. Under **Balance other jobs in**, select one or more other AZs. Ops Manager balances instances
of jobs with more than one instance across the AZs that you specify.

1. From the **Network** dropdown, select the runtime network that you created when configuring
the BOSH Director tile.

1. Click **Save**.


## <a id='exporter-config'></a> (Optional) Configure TAS for VMs Metric Exporter VMs

In the **TAS Exporter Configuration** pane, you configure static IP addresses for the metric
exporter VMs that collect metrics from the Loggregator Firehose in TAS for VMs. There are three
metric exporter VMs that each collect a single metric type from the Loggregator Firehose: counter,
gauge, or timer. You can deploy all three VMs or any subset thereof. After generating these
metrics, the metric exporter VMs convert them to a Prometheus exposition format on a secured
endpoint.

You can also deploy two other VMs: the TAS for VMs service level indicator (SLI) exporter VM
and the certificate expiration metric exporter VM.

To configure the **TAS Exporter Configuration** pane:

<p class='note warning'><strong>Warning:</strong> The IP addresses you configure in the <strong>TAS
  Exporter Configuration</strong> pane must not be within the reserved IP ranges you configured
  in the BOSH Director tile.</p>

1. Select **TAS Exporter Configuration**.

1. (Optional) For **Static IP for TAS Counter Exporter VM**, enter a valid static IP address
that you want to reserve for the counter metric exporter VM.

1. (Optional) For **Static IP for TAS Gauge Exporter VM**, enter a valid static IP address
that you want to reserve for the gauge metric exporter VM.

1. (Optional) For **Static IP for TAS Timer Exporter VM**, enter a valid static IP address
that you want to reserve for the timer metric exporter VM.

1. (Optional) For **Static IP for TAS SLI Exporter VM**, enter a valid static IP address that
you want to reserve for the TAS for VMs SLI exporter VM. The TAS for VMs SLI exporter VM generates
SLIs that allow you to monitor whether the core functions of the Cloud Foundry Command-Line
Interface (cf CLI) are working as expected. The cf CLI enables developers to create and manage
apps through TAS for VMs. For more information, see [TAS for VMs SLI Exporter VM](../metrics.html#pas-sli-exporter)
in _Healthwatch Metrics_.

1. (Optional) For **Static IP for Cert Expiration Exporter VM**, enter a valid static IP address
that you want to reserve for the certificate expiration metric exporter VM. The certificate
expiration metric exporter VM collects metrics that show when certificates in your Ops Manager
deployment are due to expire. For more information, see [Certificate Expiration Metric Exporter
VM](../metrics.html#cert-expiration-exporter) in _Healthwatch Metrics_ and [Monitoring Certificate
Expiration](optional-config/certificate-monitoring.html).
  <p class='note'><strong>Note:</strong> If you have both Healthwatch Exporter for TAS for
    VMs and Healthwatch Exporter for TKGI installed on the same foundation, scale the certificate
    expiration metric exporter VM to zero instances in the <strong>Resource Config</strong>
    pane in one of the Healthwatch Exporter tiles. Otherwise, the two certificate expiration
    metric exporter VMs create redundant sets of metrics.</p>

1. (Optional) If your Ops Manager deployment uses self-signed certificates, enable the **Skip
SSL Validation for Cert Expiration** checkbox to enable the certificate expiration metric exporter
VM to communicate with your Ops Manager deployment. This checkbox is disabled by default.

1. Click **Save**.


## <a id='bosh-exporter'></a> Configure the BOSH Health Metric Exporter VM

In the **BOSH Health Exporter Configuration** pane, you configure the AZ and VM type of the
BOSH health metric exporter VM. Healthwatch Exporter for TAS for VMs deploys the BOSH health
metric exporter VM, which creates a BOSH deployment called `bosh-health` every ten minutes.
The `bosh-health` deployment deploys another VM, `bosh-health-check`, that runs a suite of
SLI tests to validate the functionality of the BOSH Director. After the SLI tests are complete,
the BOSH health metric exporter VM collects the metrics from the `bosh-health-check` VM, then
deletes the `bosh-health` deployment and the `bosh-health-check` VM. For more information,
see [BOSH Health Metric Exporter VM](../metrics.html#bosh-health-exporter) in _Healthwatch
Metrics_.

To configure the **BOSH Health Exporter Configuration** pane:

1. Select **BOSH Health Exporter Configuration**.

1. Under **BOSH Health Check Availability Zone**, select the AZ on which you want Healthwatch
Exporter for TAS for VMs to deploy the BOSH health metric exporter VM.

1. Under **BOSH Health Check VM Type**, select from the dropdown the type of VM you want Healthwatch
Exporter for TAS for VMs to deploy.

1. Click **Save**.

<p class='note'><strong>Note:</strong> If you have both Healthwatch Exporter for TAS for VMs
  and Healthwatch Exporter for TKGI installed on the same foundation, scale the BOSH health
  metric exporter VM to zero instances in the <strong>Resource Config</strong> pane in one
  of the Healthwatch Exporter tiles. Otherwise, the two sets of BOSH health metric exporter
  VM metrics cause a <code>401</code> error in your BOSH Director deployment, and one set of
  metrics reports that the BOSH Director is down in the Grafana UI. For more information, see
  <a href="../troubleshooting.html#bosh-health-metric-errors">BOSH Health Metrics Cause Errors
  When Two Healthwatch Exporter Tiles Are Installed</a> in <em>Troubleshooting Healthwatch</em>.</p>


## <a id='bosh-deployments'></a> (Optional) Configure the BOSH Deployment Metric Exporter VM

In the **Bosh Deployments Exporter Configuration** pane, you configure the authentication credentials
and a static IP address for the BOSH deployment metric exporter VM. This VM checks every 30
seconds whether any BOSH deployments other than the one created by the BOSH health metric exporter
VM are running. For more information, see [BOSH Deployment Metric Exporter VM](../metrics.html#bosh-deployments-exporter)
in _Healthwatch Metrics_.

To configure the **Bosh Deployments Exporter Configuration** pane:

1. Select **Bosh Deployments Exporter Configuration**.

1. (Optional) For **Bosh Client Username and Secret**, you must create a new UAA client to
enable the BOSH deployment metric exporter VM to access the BOSH Director VM.
    1. Navigate to the Ops Manager Installation Dashboard.
    1. Click the **BOSH Director** tile.
    1. Select the **Status** tab.
    1. Record the **BOSH Director IP Address**.
    1. Select the **Credentials** tab.
    1. In the rows for **Uaa Login Client Credentials** and **Uaa Admin User Credentials**,
    click **Link to Credential**.
    1. Record the client username and secret for **Uaa Login Client Credentials** and **Uaa
    Admin User Credentials**.
    1. SSH into the Ops Manager VM by following the procedure in [Log In to the Ops Manager
    VM with SSH](https://docs.pivotal.io/ops-manager/install/trouble-advanced.html#ssh) in
    _Advanced Troubleshooting with the BOSH CLI_ in the Ops Manager documentation.
    1. Target the UAA instance for the BOSH Director by running:

        ```
        uaac target https://BOSH-DIRECTOR-IP:8443 --skip-ssl-validation
        ```
        Where `BOSH-DIRECTOR-IP` is the IP address of your BOSH Director instance that you
        recorded from the **Status** tab in the BOSH Director tile in a previous step.
    1. Log in to the UAA instance by running:

        ```
        uaac token owner get login -s UAA-LOGIN-CLIENT-SECRET
        ```
        Where `UAA-LOGIN-CLIENT-SECRET` is the UAA login client secret you recorded from the
        **Credentials** tab in the BOSH Director tile in a previous step.
    1. When prompted, enter the UAA admin client username and secret you recorded in a previous
    step.
    1. Create a UAA client for the BOSH deployment metric exporter VM by running:

        ```
        uaac client add CLIENT-USERNAME \
          --secret CLIENT-SECRET \
          --authorized_grant_types client_credentials,refresh_token \
          --authorities bosh.read \
          --scope bosh.read
        ```
        Where:
        * `CLIENT-USERNAME` is the client username you want to set for the UAA client.
        * `CLIENT-SECRET` is the client secret you want to set for the UAA client.
    1. Return to the **Bosh Deployments Exporter Configuration** pane in the Healthwatch Exporter
    for TAS for VMs tile.
    1. Enter the username and secret for the UAA client you just created in **Bosh Client Username
    and Secret**.

1. (Optional) For **Static IP for Bosh Deployments Exporter VM**, enter a valid static IP address
that you want to reserve for the BOSH deployment metric exporter VM. This IP address must not
be within the reserved IP ranges you configured in the BOSH Director tile.

1. Click **Save**.

<p class='note'><strong>Note:</strong> If you have both Healthwatch Exporter for TAS for VMs
  and Healthwatch Exporter for TKGI installed on the same foundation, scale the BOSH deployment
  metric exporter VM to zero instances in the <strong>Resource Config</strong> pane in one
  of the Healthwatch Exporter tiles. Otherwise, the two BOSH deployment metric exporter VMs
  create redundant sets of metrics.</p>


## <a id='errands'></a> (Optional) Configure Errands

Errands are scripts that Ops Manager runs automatically when it installs or uninstalls a product,
such as a new version of Healthwatch Exporter for TAS for VMs. There are two types of errands:
_post-deploy errands_ run after the product is installed, and _pre-delete errands_ run before
the product is uninstalled.

By default, Ops Manager always runs all errands.

In the **Errands** pane, you can select **On** to always run an errand or **Off** to never
run it.

For more information about how Ops Manager manages errands, see [Managing Errands in Ops Manager]
(https://docs.pivotal.io/ops-manager/install/managing_errands.html) in the Ops Manager documentation.

To configure the **Errands** pane:

1. Select **Errands**.

1. (Optional) Choose whether to always run or never run the following errands:
    * **Smoke Tests:** Verifies that the metric exporter VMs are running.
    * **Cleanup:** Deletes any existing BOSH deployments created by the BOSH health metric
    exporter VM for running SLI tests.
    * **Remove CF SLI User:** Deletes the user account that the TAS for VMs SLI exporter VM
    creates to run the TAS for VMs SLI test suite. For more information, see [TAS for VMs SLI
    Exporter VM](../metrics.html#pas-sli-exporter) in _Healthwatch Metrics_.

1. Click **Save**.


## <a id='syslog'></a> (Optional) Configure Syslog

In the **Syslog** pane, you can configure system logging in Healthwatch Exporter for TAS for
VMs to forward log messages from tile component VMs to an external destination for troubleshooting,
such as a remote server or external syslog aggregation service.

To configure the **Syslog** pane:

1. Select **Syslog**.

1. (Optional) Under **Do you want to configure Syslog forwarding?**, select one of the following
options:
    * **No, do not forward Syslog:** Disables syslog forwarding.
    * **Yes:** Enables syslog forwarding and allows you to edit the configuration fields described
    below.

1. For **Address**, enter the IP address or DNS domain name of your external destination.

1. For **Port**, enter a port on which your external destination listens.

1. For **Transport Protocol**, select **TCP** or **UDP** from the dropdown. This determines
which transport protocol Healthwatch Exporter for TAS for VMs uses to forward system logs to
your external destination.

1. (Optional) To transmit logs over TLS:
  1. Select the **Enable TLS** checkbox. This checkbox is disabled by default.
  1. For **Permitted Peer**, enter either the name or SHA1 fingerprint of the remote peer.
  1. For **SSL Certificate**, enter the SSL certificate for your external destination.

1. (Optional) For **Queue Size**, specify the number of log messages Healthwatch Exporter for
TAS for VMs can hold in a buffer at a time before sending them to your external destination.
The default value is `100000`.

1. (Optional) To forward debug logs to your external destination, enable the **Forward Debug
Logs** checkbox. This checkbox is disabled by default.

1. (Optional) To specify a custom syslog rule, enter it in **Custom rsyslog configuration**
in RainerScript syntax. For more information about custom syslog rules, see [Customizing Platform
Log Forwarding](https://docs.pivotal.io/application-service/operating/custom-syslog-rules.html)
in the TAS for VMs documentation. For more information about RainerScript syntax, see the [rsyslog]
(https://www.rsyslog.com/doc/v8-stable/rainerscript/index.html) documentation.

1. Click **Save Syslog Settings**.


## <a id='resource-config'></a> (Optional) Configure Resources

In the **Resource Config** pane, you can scale VMs in Healthwatch Exporter for TAS for VMs
up or down according to the needs of your deployment, as well as associate load balancers with
a group of VMs. For example, you can scale the persistent disk size of a metric exporter VM
to enable longer data retention.

To configure the **Resource Config** pane:

1. Select **Resource Config**.

1. (Optional) To scale a job, select an option from the dropdown for the resource you want
to modify:
    * **Instances:** Configures the number of instances each job has.
    * **VM Type:** Configures the type of VM used in each instance.
    * **Persistent Disk Type:** Configures the amount of persistent disk space to allocate
    to the job.

1. (Optional) To add a load balancer to a job:
  1. Click the icon next to the job name.
  1. For **Load Balancers**, enter the name of your load balancer.
  1. Ensure that the **Internet Connected** checkbox is disabled. Enabling this checkbox gives
  VMs a public IP address that enables outbound Internet access.

1. (Optional) The instance count for the SVM Forwarder VM is set to `0` by default. This VM
emits Healthwatch-generated super value metrics (SVMs) into the Loggregator Firehose. To enable
the SVM Forwarder VM, increase the instance count by selecting from the **Instances** dropdown.
You do not need to enable this VM unless your foundation uses a Pivotal Healthwatch v1.8 or
earlier integration and a third-party nozzle to export the SVMs to an external system, such
as a remote server or a syslog aggregation service. For more information about the SVM Forwarder
VM, see [SVM Forwarder VM - Platform Metrics](../metrics.html#svm-forwarder-platform) and [SVM
Forwarder VM - Healthwatch Component Metrics](../metrics.html#svm-forwarder-components) in
_Healthwatch Metrics_.
  <p class='note'><strong>Note:</strong> If you installed the Healthwatch Exporter for TAS
    for VMs tile before installing the Healthwatch tile, you may need to re-deploy Healthwatch
    Exporter for TAS for VMs after enabling the SVM Forwarder VM. For more information, see
    <a href="#apply-changes">Deploy Healthwatch Exporter for TAS for VMs</a> below.</p>

1. (Optional) Healthwatch Exporter for TAS for VMs deploys the counter, gauge, and timer metric
exporter VMs by default. If you do not want to collect all of these metric types, set the instance
count for the VMs associated with the metrics you do not want to collect to `0`.

1. Click **Save**.


## <a id='apply-changes'></a> Deploy Healthwatch Exporter for TAS for VMs

To complete your installation of the Healthwatch Exporter for TAS for VMs tile:

1. Return to the Ops Manager Installation Dashboard.

1. Click **Review Pending Changes**.

1. Click **Apply Changes**.

For more information, see [Reviewing Pending Product Changes](https://docs.pivotal.io/ops-manager/install/review-pending-changes.html)
in the Ops Manager documentation.


## <a id='scrape-job-config'></a> Configure a Scrape Job for Healthwatch Exporter for TAS for VMs

After you have successfully deployed Healthwatch Exporter for TAS for VMs, you must configure
a scrape job in the Prometheus instance that exists within your metrics monitoring system.
Follow the procedure in one of the following sections, depending on which monitoring system
you use:

* If you monitor metrics using the Healthwatch tile on an Ops Manager foundation, see [Configure
a Scrape Job for Healthwatch Exporter for TAS for VMs in Healthwatch](#scraping-healthwatch)
below.
  <p class='note'><strong>Note:</strong> You only need to configure a scrape job for installations
  of Healthwatch Exporter for TAS for VMs that are not on the same foundation as your Healthwatch
  tile. The Prometheus instance in the Healthwatch tile automatically discovers and scrapes
  Healthwatch Exporter tiles that are installed on the same foundation as the Healthwatch tile.</p>

* If you monitor metrics using a service or database located outside your Ops Manager foundation,
such as an external TSDB or an installation of Healthwatch on the TKGI Control Plane, see [Configure
a Scrape Job for Healthwatch Exporter for TAS for VMs in an External Monitoring System](#scraping-external)
below.

### <a id='scraping-healthwatch'></a> Configure a Scrape Job for Healthwatch Exporter for TAS for VMs in Healthwatch

To configure a scrape job for Healthwatch Exporter for TAS for VMs in the Healthwatch tile
on your Ops Manager foundation, see [(Optional) Configure Prometheus](configuring-healthwatch.html#prometheus)
in _Configuring Healthwatch_.

### <a id='scraping-external'></a> Configure a Scrape Job for Healthwatch Exporter for TAS for VMs in an External Monitoring System

To configure a scrape job for Healthwatch Exporter for TAS for VMs in a service or database
that is located outside your Ops Manager foundation:

1. Open network communication paths from your external service or database to the metric exporter
VMs in Healthwatch Exporter for TAS for VMs. The procedure to open these network paths differs
depending on your Ops Manager foundation's IaaS. For a list of TCP ports used by each metric
exporter VM, see [Required Networking Rules for Healthwatch Exporter for TAS for VMs](../architecture.html#network-rules-tas)
in _Reference Architecture_.

1. In the `scrape_config` section of the Prometheus configuration file, create a scrape job
for your Ops Manager foundation. Under `static_config`, specify the TCP ports of each metric
exporter VM as static targets for the IP address of your external service or database. For
example:

    ```
    job_name: foundation-1
    metrics_path: /metrics
    scheme: https
    static_configs:
    - targets:
      - "1.2.3.4:8443"
      - "1.2.3.4:25555"
      - "1.2.3.4:443"
      - "1.2.3.4:8082"
    ```
    For more information, see [&#60;scrape_config>](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config)
    and [&#60;static_config>](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#static_config)
    in _Configuration_ in the Prometheus documentation.
