---
title: PCF Healthwatch Metrics
owner: PCF Healthwatch
---

This topic lists the super metrics created by Pivotal Cloud Foundry (PCF) Healthwatch. 

<p class="note"><strong>Note</strong>: For external monitoring consumers, PCF Healthwatch forwards the metrics it creates into the Loggregator Firehose.</p> 

In this topic, you can also find information about the existing PCF platform component and BOSH VM metrics used by PCF Healthwatch.

##<a id='cli'></a>Healthwatch: Cloud Foundry CLI Health

The Cloud Foundry command line interface (CLI) enables developers to create and manage PCF apps. PCF Healthwatch executes a continuous test suite validating the core app developer functions of the CLI. Running a continuous validation test suite is often significantly more meaningful to reassuring functionality than monitoring for trends in metrics alone.

See the table below for information on generated metrics related to the Cloud Foundry CLI Health smoke tests.

<table class="nice">
<tr>
  <th>Test</th>
  <th>Metric</th>
  <th>Frequency</th>
  <th>Description</th>
</tr>
<tr>
  <td>Can login</td>
  <td><code>healthwatch.health.check.cliCommand.login</code> and <code>healthwatch.health.check.cliCommand.login.timeout</code></td>
  <td>5 min</td>
  <td><code>1</code> = pass or <code>0</code> = fail</td>
</tr>
<tr>
  <td>Can push</td>
  <td><code>healthwatch.health.check.cliCommand.push</code> and <code>healthwatch.health.check.cliCommand.push.timeout</code></td>
  <td>5 min</td>
  <td><code>1</code> = pass, <code>0</code> = fail, or <code>-1</code> = test did not run</td>
</tr>
<tr>
  <td>Can start</td>
  <td><code>healthwatch.health.check.cliCommand.start</code> and <code>healthwatch.health.check.cliCommand.start.timeout</code></td>
  <td>5 min</td>
  <td><code>1</code> = pass, <code>0</code> = fail, or <code>-1</code> = test did not run</td>
</tr>
<tr>
  <td>Receiving logs</td>
  <td><code>healthwatch.health.check.cliCommand.logs</code> and <code>healthwatch.health.check.cliCommand.logs.timeout</code></td>
  <td>5 min</td>
  <td><code>1</code> = pass, <code>0</code> = fail, or <code>-1</code> test did not run</td>
</tr>
<tr>
  <td>Can stop</td>
  <td><code>healthwatch.health.check.cliCommand.stop</code> and <code>healthwatch.health.check.cliCommand.stop.timeout</code></td>
  <td>5 min</td>
  <td><code>1</code> = pass, <code>0</code> = fail, or <code>-1</code> test did not run</td>
</tr>
<tr>
  <td>Can delete</td>
  <td><code>healthwatch.health.check.cliCommand.delete</code> and <code>healthwatch.health.check.cliCommand.delete.timeout</code></td>
  <td>5 min</td>
  <td><code>1</code> = pass, <code>0</code> = fail, or <code>-1</code> test did not run</td>
</tr>
<tr>
  <td>Test app push time</td>
  <td><code>healthwatch.health.check.cliCommand.pushTime</code></td>
  <td>5 min</td>
  <td>Time in ms</td>
</tr>
<tr>
  <td>Overall smoke test battery result</td>
  <td><code>healthwatch.health.check.cliCommand.success</code></td>
  <td>5 min</td>
  <td><code>1</code> = pass or <code>0</code> = fail</td>
</tr>
<tr>
  <td>Overall smoke test battery run time</td>
  <td><code>healthwatch.health.check.cliCommand.duration</code></td>
  <td>5 min</td>
  <td>Time in ms</td>
</tr>
</table>

<p class="note"><strong>Note</strong>: Timeout metrics are written only when a timeout occurs. Their value is always zero.</p>

<p class="note"><strong>Note</strong>: PCF Healthwatch runs this test suite in the <strong>system</strong> org and the <strong>platform_monitoring</strong> space.</p>

##<a id='opsman'></a>Healthwatch: Ops Manager Health

Issues with Ops Manager health can impact an operator's ability to perform an upgrade or to rescale the PCF platform when necessary. Therefore, it is recommended to continuously monitor Ops Manager availability. PCF Healthwatch executes this check as a part of its test suite.

<table class="nice">
<tr>
  <th>Test</th>
  <th>Metric</th>
  <th>Frequency</th>
  <th>Description</th>
</tr>
<tr>
  <td>Ops Manager availability</td>
  <td><code>healthwatch.health.check.OpsMan.available</code></td>
  <td>1 min</td>
  <td><code>1</code> = pass or <code>0</code> = fail</td>
</tr>
</table>

##<a id='canaryapp'></a>Healthwatch: Canary App Health

App availability and responsiveness issues can result in significant end user impacts. PCF Healthwatch uses Apps Manager as a canary app and continuously checks its health. Because of the functions Apps Manager provides, Pivotal recommends it as a canary for insight into the performance of other apps on the foundation.

<table class="nice">
<tr>
  <th>Test</th>
  <th>Metric</th>
  <th>Frequency</th>
  <th>Description</th>
</tr>
<tr>
  <td>Apps Manager availability</td>
  <td><code>healthwatch.health.check.CanaryApp.available</code></td>
  <td>1 min</td>
  <td><code>1</code> = pass or <code>0</code> = fail</td>
</tr>
<tr>
  <td>Aps Manager response time</td>
  <td><code>healthwatch.health.check.CanaryApp.responseTime</code></td>
  <td>1 min</td>
  <td>Time in ms</td>
</tr>
</table>

##<a id='bosh-director'></a>Healthwatch: BOSH Director Health

Losing the BOSH Director does not significantly impact the experience of PCF end users. However, this issue means a loss of resiliency for BOSH-managed VMs. It is recommended to continuously monitor the BOSH Director health. PCF Healthwatch executes this check as a part of its test suite.

<table class="nice">
<tr>
  <th>Test</th>
  <th>Metric</th>
  <th>Frequency</th>
  <th>Description</th>
</tr>
<tr>
  <td>BOSH Director health</td>
  <td><code>healthwatch.health.check.bosh.director.success</code> and <code>healthwatch.health.check.bosh.director.timeout</code></td>
  <td>10 min</td>
  <td><code>1</code> = pass or <code>0</code> = fail</td>
</tr>
</table>

<p class="note"><strong>Note</strong>: The timeout metric is written if a deploy or delete task takes more than 10 minutes.</p>

<p class="note"><strong>Note</strong>: PCF Healthwatch deploys, stops, starts, and deletes a VM named <code>bosh-health-check</code> as part of this test suite.</p>

##<a id='loss-rates'></a>Healthwatch: Logging Performance Loss Rates

This section lists metrics used to monitor Loggregator, the PCF component responsible for logging.

###<a id='firehose-loss'></a>Firehose Loss Rate

This derived metric is recommended for automating and monitoring platform scaling. Two versions of the metric (per minute and per hour) are used to monitor the Loggregator Firehose.

<table class="nice">
<tr>
  <th>Reports</th>
  <th>Metric</th>
  <th>Description</th>
</tr>
<tr>
  <td>Firehose loss rate</td>
  <td><code>healthwatch.Firehose.LossRate.1H</code> and <code>healthwatch.Firehose.LossRate.1M</code></td>
  <td>Loss rate per minute and per hour</td>
</tr>
</table>

###<a id='adapter-loss'></a>Adapter Loss Rate

This derived metric is recommended for automating and monitoring platform scaling. The metric is used to monitor the Scalable Syslog feature of Loggregator.

<table class="nice">
<tr>
  <th>Reports</th>
  <th>Metric</th>
  <th>Description</th>
</tr>
<tr>
  <td>Adapter loss rate (syslog drain performance)</td>
  <td><code>healthwatch.ScalableSyslog.Adapter.LossRate.1M</code></td>
  <td>Loss rate per minute</td>
</tr>
</table>

###<a id='rlp-loss'></a>Reverse Log Proxy Loss Rate

This derived metric is recommended for automating and monitoring platform scaling. The metric is used to monitor the Scalable Syslog feature of Loggregator.

<table class="nice">
<tr>
  <th>Reports</th>
  <th>Metric</th>
  <th>Description</th>
</tr>
<tr>
  <td>Reverse Log Proxy loss rate (syslog drain performance)</td>
  <td><code>healthwatch.ScalableSyslog.RLP.LossRate.1M</code></td>
  <td>Loss rate per minute</td>
</tr>
</table>

##<a id='capacity'></a>Healthwatch: Percentage of Capacity Available

This section lists metrics used to monitor the total percentage of available memory, disk, and cell container capacity.

###<a id='memory'></a>Percentage of Memory Available

This derived metric is recommended for automating and monitoring platform scaling.

<table class="nice">
<tr>
  <th>Reports</th>
  <th>Metric</th>
  <th>Description</th>
</tr>
<tr>
  <td>Available memory</td>
  <td><code>healthwatch.Diego.TotalPercentageAvailableMemoryCapacity.5M</code></td>
  <td>Percentage of available memory (averaged over last 5 min)</td>
</tr>
</table>

###<a id='disk'></a>Percentage of Disk Available

This derived metric is recommended for automating and monitoring platform scaling.

<table class="nice">
<tr>
  <th>Reports</th>
  <th>Metric</th>
  <th>Description</th>
</tr>
<tr>
  <td>Available disk</td>
  <td><code>healthwatch.Diego.TotalPercentageAvailableDiskCapacity.5M</code></td>
  <td>Percentage of available disk (averaged over last 5 min)</td>
</tr>
</table>

###<a id='cell-container'></a>Percentage of Cell Container Capacity Available 

This derived is recommended for automating and monitoring platform scaling.

<table class="nice">
<tr>
  <th>Reports</th>
  <th>Metric</th>
  <th>Description</th>
</tr>
<tr>
  <td>Available cell container capacity</td>
  <td><code>healthwatch.Diego.TotalPercentageAvailableContainerCapacity.5M</code></td>
  <td>Percentage of available cell container capacity (averaged over last 5 min)</td>
</tr>
</table>

##<a id='bosh-deployment'></a>Healthwatch: BOSH Deployment Occurrence

Monitoring BOSH deployment occurrence adds context to related data, such as VM (job) health.

**Limitation**: A BOSH deployment start or complete event can be determined. However, you cannot currently know to which VMs it is occurring.

<table class="nice">
<tr>
  <th>Reports</th>
  <th>Metric</th>
  <th>Frequency</th>
  <th>Description</th>
</tr>
<tr>
  <td>BOSH deployment occurrence</td>
  <td><code>healthwatch.bosh.deployment</code></td>
  <td>30 sec</td>
  <td><code>1</code> = a running deployment or <code>0</code> = not a running deployment</td>
</tr>
</table>

##<a id='other-metrics'></a>Other Exising Platform Metrics Used

This section lists the existing platform metrics used by PCF Healthwatch. For more information about these metrics, see [Key Performance Indicators](https://docs.pivotal.io/pivotalcf/1-11/monitoring/kpi.html) and [Key Capacity Scaling Indicators](https://docs.pivotal.io/pivotalcf/1-11/monitoring/key-cap-scaling.html).

###<a id='job-health'></a>Job Health

The Job Health metric is used for every VM in the CF deployment, and it is provided through BOSH. This does not include additional deployments, such as RabbitMQ or Redis.

<table class="nice">
<tr>
  <th>Reports</th>
  <th>Metric</th>
  <th>Description</th>
</tr>
<tr>
  <td>Job health</td>
  <td><code>system.healthy</code></td>
  <td><code>1</code> = system is healthy or <code>0</code> = system is not healthy</td>
</tr>
</table>

###<a id='job-vitals'></a>Job Vitals

The Job Vitals metrics are written for core ERT jobs, and they are provided through BOSH. This does not include additional deployments, such as RabbitMQ or Redis.

<table class="nice">
<tr>
  <th>Reports</th>
  <th>Metric</th>
  <th>Description</th>
</tr>
<tr>
  <td>CPU utilization</td>
  <td><code>system.cpu.user</code></td>
  <td>Percentage of CPU used</td>
</tr>
<tr>
  <td>Memory utilization</td>
  <td><code>system.mem.percent</code></td>
  <td>Percentage of system memory used</td>
</tr>
<tr>
  <td>Disk utilization</td>
  <td><code>system.disk.system.percent</code></td>
  <td>Percentage of system disk used</td>
</tr>
<tr>
  <td>Persistent disk utilization</td>
  <td><code>system.disk.persistent.percent</code></td>
  <td>Percentage of persistent disk used</td>
</tr>
<tr>
  <td>Ephemeral disk utilization</td>
  <td><code>system.disk.ephemeral.percent</code></td>
  <td>Percentage of ephemeral disk used</td>
</tr>
</table>

###<a id='kpis-capacity'></a>Diego Cell Capacity

The Capacity metrics are used to monitor the amount of memory, disk, and container capacity available for Diego cell(s).

<table class="nice">
<tr>
  <th>Reports</th>
  <th>Metric</th>
  <th>Description</th>
</tr>
<tr>
  <td>Available memory</td>
  <td><code>rep.CapacityRemainingMemory</code></td>
  <td>Amount of memory (MiB) available for a Diego cell to allocate to containers</td>
</tr>
<tr>
  <td>Total memory</td>
  <td><code>rep.CapacityTotalMemory</code></td>
  <td>Total amount of memory (MiB) available for this cell to allocate to containers</td>
</tr>
<tr>
  <td>Available disk</td>
  <td><code>rep.CapacityRemainingDisk</code></td>
  <td>Amount of disk (MiB) available for a Diego cell to allocate to containers</td>
</tr>
<tr>
  <td>Total disk</td>
  <td><code>rep.CapacityTotalDisk</code></td>
  <td>Total amount of disk (MiB) available for this cell to allocate to containers</td>
</tr>
<tr>
  <td>Available container capacity</td>
  <td><code>rep.CapacityRemainingContainers</code></td>
  <td>Remaining number of containers this cell can host</td>
</tr>
<tr>
  <td>Total container capacity</td>
  <td><code>rep.CapacityTotalContainers</code></td>
  <td>Total number of containers this cell can host</td>
</tr>
</table>

###<a id='app-instances'></a>Application Instances

The Application Instances metrics are used to monitor the health of application instances (AIs). For more information about the lifecycle of an app container and crash events, see [Crash Events](https://docs.pivotal.io/pivotalcf/1-11/devguide/deploy-apps/app-lifecycle.html#crash-events).

<table class="nice">
<tr>
  <th>Reports</th>
  <th>Metric</th>
  <th>Description</th>
</tr>
<tr>
  <td>Current running AIs and change in running AIs</td>
  <td><code>bbs.LRPsRunning</code></td>
  <td>Total number of LRP instances running on Diego cells</td>
</tr>
<tr>
  <td>Crashed AIs</td>
  <td><code>bbs.CrashedActualLRPs</code></td>
  <td>Total number of LRP instances that have crashed in a deployment</td>
</tr>
<tr>
  <td>Missing AIs</td>
  <td><code>bbs.LRPsMissing</code></td>
  <td>Total number of LRP instances that are desired but have no record in the BBS</td>
</tr>
<tr>
  <td>Extra AIs</td>
  <td><code>bbs.LRPsExtra</code></td>
  <td>Total number of LRP instances that are no longer desired but still have a BBS record</td>
</tr>
<tr>
  <td>Auctioneer AI starts</td>
  <td><code>auctioneer.AuctioneerLRPAuctionsStarted</code></td>
  <td>Number of LRP instances that the Auctioneer successfully placed on Diego cells</td>
</tr>
<tr>
  <td>Auctioneer AI failures</td>
  <td><code>auctioneer.AuctioneerLRPAuctionsFailed</code></td>
  <td>Number of LRP instances that the Auctioneer failed to place on Diego cells</td>
</tr>
<tr>
  <td>Auctioneer task placement failures</td>
  <td><code>auctioneer.AuctioneerTaskAuctionsFailed</code></td>
  <td>Number of Tasks that the Auctioneer failed to place on Diego cells</td>
</tr>
</table>

###<a id='diego-health'></a>Diego Health

The Diego health and performance metrics are used to monitor core Diego functionality.

<table class="nice">
<tr>
  <th>Reports</th>
  <th>Metric</th>
  <th>Description</th>
</tr>
<tr>
  <td>BBS time to handle requests</td>
  <td><code>bbs.RequestLatency</code></td>
  <td>Time in ns that the BBS took to handle requests aggregated across all its API endpoints</td>
</tr>
<tr>
  <td>BBS time to run LRP convergence</td>
  <td><code>bbs.ConvergenceLRPDuration</code></td>
  <td>Time in ns that the BBS took to run its LRP convergence pass</td>
</tr>
<tr>
  <td>Auctioneer time to fetch Cell state</td>
  <td><code>auctioneer.AuctioneerFetchStatesDuration</code></td>
  <td>Time in ns that the Auctioneer took to fetch state from all the Diego cells when running its auction</td>
</tr>
<tr>
  <td>Route Emitter time to sync</td>
  <td><code>route_emitter.RouteEmitterSyncDuration</code></td>
  <td>Time in ns that the active route-emitter took to perform its synchronization pass</td>
</tr>
<tr>
  <td>Cell Rep time to sync</td>
  <td><code>rep.RepBulkSyncDuration</code></td>
  <td>Time in ns that the Diego Cell Rep took to sync the ActualLRPs that it claimed with its actual Garden containers</td>
</tr>
<tr>
  <td>Locket active presences</td>
  <td><code>locket.ActivePresences</code></td>
  <td>Total count of active presences<sup>&#42;</sup></td>
</tr>
<tr>
  <td>Locket active locks</td>
  <td><code>locket.ActiveLocks</code></td>
  <td>Total count of how many locks the system components are holding</td>
</tr>
  <tr>
  <td>Diego Cell health check</td>
  <td><code>rep.UnhealthyCell</code></td>
  <td><code>0</code> = healthy Cell or <code>1</code> = unhealthy Cell<sup>&#8224;</sup></td>
</tr>
<tr>
  <td>Diego and Cloud Controller synched check</td>
  <td><code>bbs.Domain.cf-apps</code></td>
  <td>Indicates whether the <code>cf-apps</code> domain is up-to-date<sup>&#135;</sup></td>
</tr>
</table>

<sup>&#42;</sup> Presences are defined as the registration records that the Cells maintain to advertise themselves to the platform.

<sup>&#8224;</sup> The Diego cell periodically checks its health against the Garden backend.

<sup>&#135;</sup> This means that Cloud Foundry app requests from Cloud Controller are synchronized to `bbs.LRPsDesired` (Diego-desired AIs) for execution.

###<a id='logging'></a>Logging Performance

The Loggregator Firehose and Scalable Syslog metrics are used to monitor PCF logging performance.

<table class="nice">
<tr>
  <th>Reports</th>
  <th>Metric</th>
  <th>Description</th>
</tr>
<tr>
  <td>Firehose throughput</td>
  <td><code>DopplerServer.listeners.totalReceivedMessageCount</code> (+ <code>loggregator.doppler.ingress</code> in PCF v1.12)</td>
  <td>Total number of messages received across all Doppler listeners</td>
</tr>
<tr>
  <td>Firehose dropped messages</td>
  <td><code>DopplerServer.doppler.shedEnvelopes</code> (+ <code>loggregator.doppler.dropped</code> in PCF v1.12)</td>
  <td>Total number of messages intentionally dropped by Doppler due to back pressure</td>
</tr>
<tr>
  <td>Syslog drain binding count</td>
  <td><code>scalablesyslog.scheduler.drains</code></td>
  <td>Number of scalable syslog drain bindings</td>
</tr>
</table>

###<a id='router'></a>Router

The Router metrics are used to monitor the health and performance of the Gorouter.

<table class="nice">
<tr>
  <th>Reports</th>
  <th>Metric</th>
  <th>Description</th>
</tr>
<tr>
  <td>Router throughput</td>
  <td><code>gorouter.total_requests</code></td>
  <td>Lifetime number of requests completed by the Gorouter VM</td>
</tr>
<tr>
  <td>Router latency</td>
  <td><code>gorouter.latency</code></td>
  <td>Time (ms) the Gorouter takes to handle requests to its app endpoints</td>
</tr>
<tr>
  <td>Router jobs CPU</td>
  <td><code>system.cpu.user</code></td>
  <td>CPU utilization of the Gorouter job(s) as reported by BOSH</td>
</tr>
<tr>
  <td>502 bad gateways</td>
  <td><code>gorouter.bad_gateways</code></td>
  <td>Lifetime number of bad gateways, or 502 responses, from the Gorouter itself</td>
</tr> 
<tr>
  <td>All 5XX errors</td>
  <td><code>gorouter.responses.5xx</code></td>
  <td>Lifetime number of requests completed by the GoRouter VM for HTTP status family 5xx, server errors</td>
</tr>
<tr>
  <td>Number of routes registered</td>
  <td><code>gorouter.total_routes</code></td>
  <td>Current total number of routes registered with the GoRouter</td>
</tr>
<tr>
  <td>Router File Descriptors</td>
  <td><code>gorouter.file_descriptors</code></td>
  <td>The number of file descriptors currently used by the Gorouter job<sup>&#42;</sup></td>
</tr> 
  <tr>
  <td>Router Exhausted Connections</td>
  <td><code>gorouter.backend_exhausted_conns</code></td>
  <td>The lifetime number of requests that have been rejected by the Gorouter VM due to the `Max Connections Per Backend` limit being reached across all tried backends<sup>&#42;</sup></td>
</tr>
<tr>
  <td>Time since last route registered</td>
  <td><code>gorouter.ms_since_last_registry_update</code></td>
  <td>Time in ms since the last route register was received</td>
</tr>
</table>

<sup>&#42;</sup> This metric is relevant to PCF v1.12 and does not appear in PCF Healthwatch if it is running on PCF v1.11.
