---
title: Monitoring PCF Healthwatch
owner: PCF Healthwatch
---

This topic explains how to monitor the health of the Pivotal Cloud Foundry (PCF) Healthwatch product itself using the metrics and Key Performance Indicators (KPIs) generated by PCF Healthwatch.

For general information about monitoring PCF, see [Monitoring Pivotal Cloud Foundry](https://docs.pivotal.io/pivotalcf/1-12/monitoring/index.html).

## <a id="aboutMetrics"></a> About Metrics

Metrics are regularly-generated log messages that report measured component states. Metrics emitted by PCF Healthwatch for the purpose of Operational Monitoring of the PCF Healthwatch product follow this type of format:
<code>
origin:"healthwatch" eventType:ValueMetric timestamp:1509638101820496694 deployment:"healthwatch-app-dev-v1-1" job:"healthwatch-forwarder" index:"097f4b1e-5ca8-4866-82d5-00883798dad4" ip:"10.0.16.29" valueMetric:<name:"healthwatch.metrics.published" value:38 unit:"count" >
</code>

All PCF Healthwatch emitted metrics have the same origin, <code>healthwatch</code>

## <a id="keyPerformanceIndicators"></a> Key Performance Indicators for PCF Healthwatch

###<a id="firehoseDisconnectMetrics"></a>Number of Healthwatch Nozzle Disconnects from Firehose
<table>
   <tr><th colspan="2" style="text-align: center;"><br/> healthwatch.ingestor.disconnects<br/><br/></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>
        Number of forced disconnects of the Healthwatch [data ingestor nozzle](architecture.html) from the firehose.<br/><br/>
        <strong>Use</strong>: An unusual increase in the number of disconnects from the Firehose typically indicates that the Healthwatch nozzle needs to be scaled to more instances. The firehose will disconnect nozzles that are being slow consumers in order to protect applications from back pressure. This metric can also spike during a PCF deployment as the Traffic Controller VMs restart, logging a disconnect.
        <br/><br/>
        A prolonged period of losing metrics as a result of disconnects could endanger the assessments Healthwatch makes from firehose-based platform metrics.
        <br/><br/>
        <strong>Origin</strong>: Firehose<br/>
        <strong>Type</strong>: Gauge<br/>
        <strong>Frequency</strong>: 60s<br/>
      </td>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>Average over last 5 minutes</td>
   </tr>
   <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: Dynamic<br/>
      <strong>Red critical</strong>: Dynamic</td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td>
         If no known deployment occurred, and the spike is sustained, increase the number of Healthwatch Ingestor instances and monitor this metric to ensure it returns to a normal state.
         <br/><br/>
         Ingestor instances can be scaled within the Healthwatch “Component Config” tile UI, or via `cf scale healthwatch-ingestor`. CF scale can more quickly resolve an issue, however the tile config should be updated so that the next deployment does not override the manual scale action.
      </td>
   </tr>
</table>

###<a id="loaderDroppedMetrics"></a>Number of Metrics Dropped by Healthwatch Data Loader
<table>
   <tr><th colspan="2" style="text-align: center;"><br/> healthwatch.loader.dropped<br/><br/></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>
        Number of metrics dropped by the Healthwatch [data loader](architecture.html), which loads incoming data into the Healthwatch Data Store.
        <br/><br/>
        <strong>Use</strong>: An unusual increase in the number of dropped metrics by the Healthwatch Loader likely indicates that this component needs to be scaled to more instances. A prolonged period of dropping metrics could endanger the assessments Healthwatch makes from firehose-based platform metrics.
        <br/><br/>
        <strong>Origin</strong>: Firehose<br/>
        <strong>Type</strong>: Gauge<br/>
        <strong>Frequency</strong>: 60s<br/>
      </td>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>Average over last 5 minutes</td>
   </tr>
   <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: Dynamic<br/>
      <strong>Red critical</strong>: Dynamic</td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td>
         Increase the number of Healthwatch Loader instances and monitor this metric to ensure it returns to a normal state.
         <br/><br/>
         Loader instances can be scaled within the Healthwatch “Component Config” tile UI, or via `cf scale healthwatch-loader`. CF scale can more quickly resolve an issue, however the tile config should be updated so that the next deployment does not override the manual scale action.
      </td>
   </tr>
</table>

###<a id="uiAvailabilityMetric"></a>Healthwatch UI Availability
<table>
   <tr><th colspan="2" style="text-align: center;"><br/> healthwatch.ui.available<br/><br/></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>
        The PCF Healthwatch [UI](architecture.html) is currently available. This assessment is made via a probe that looks for a successful response.
        <br/><br/>
        1 = available<br/>
        0 = not available, or timeout (10s)
        <br/><br/>
        <strong>Use</strong>: Indicates that the Healthwatch UI is running and available to product end users. While an issue with the UI does not impact the assessments Healthwatch is making, loss of the UI can impact user ability to visually reference these assessments, including the loss of the main dashboard display.
        <br/><br/>
        <strong>Origin</strong>: Firehose<br/>
        <strong>Type</strong>: Gauge<br/>
        <strong>Frequency</strong>: 60s<br/>
      </td>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>Average over last 5 minutes</td>
   </tr>
   <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: &le; 0.6<br/>
      <strong>Red critical</strong>: &le; 0.4</td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td>
         TBD
      </td>
   </tr>
</table>

###<a id="cliTestAvailabilityMetric"></a>CLI Health Test Availability
<table>
   <tr><th colspan="2" style="text-align: center;"><br/> health.check.cliCommand.probe.available<br/><br/></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>
        PCF Healthwatch has up-to-date results for the [CLI Command Health Test](metrics.html#cli), meaning that testing was recently available.
        <br/><br/>
        1 = available<br/>
        0 = not available, or timeout (10s)
        <br/><br/>
        An assessment of up-to-date results is made by looking for results within the configured run schedule plus timeout configured. For example, a test runner scheduled on 5 minute intervals with a 2 minute timeout, would need to show a test result within the last 7 minutes to succeed.
        <br/><br/>
        <strong>Use</strong>: Indicates that Healthwatch is assessing the current state of health for the CLI Commands. If these continuous validation tests fail to make up-to-date assessments, they are no longer a reliable warning mechanism of problems resulting in end-user impact.
        <br/><br/>
        <strong>Origin</strong>: Firehose<br/>
        <strong>Type</strong>: Gauge<br/>
        <strong>Frequency</strong>: 60s<br/>
      </td>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>Average over last 5 minutes</td>
   </tr>
   <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: &le; 0.6<br/>
      <strong>Red critical</strong>: &le; 0.4</td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td>
         TBD
      </td>
   </tr>
</table>

###<a id="canaryTestAvailabilityMetric"></a>Canary App Health Test Availability
<table>
   <tr><th colspan="2" style="text-align: center;"><br/> health.check.cliCommand.probe.available<br/><br/></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>
        PCF Healthwatch has up-to-date results for the [Canary App Health Test](metrics.html#canaryapp), meaning that testing was recently available.
        <br/><br/>
        1 = available<br/>
        0 = not available, or timeout (10s)
        <br/><br/>
        An assessment of up-to-date results is made by looking for results within the configured run schedule plus timeout configured. For example, a test runner scheduled on 5 minute intervals with a 2 minute timeout, would need to show a test result within the last 7 minutes to succeed.
        <br/><br/>
        <strong>Use</strong>: Indicates that Healthwatch is assessing the current state of health for the Canary App. If this continuous validation test fails to make up-to-date assessments, it is no longer a reliable warning mechanism of problems resulting in end-user impact.
        <br/><br/>
        <strong>Origin</strong>: Firehose<br/>
        <strong>Type</strong>: Gauge<br/>
        <strong>Frequency</strong>: 60s<br/>
      </td>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>Average over last 5 minutes</td>
   </tr>
   <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: &le; 0.6<br/>
      <strong>Red critical</strong>: &le; 0.4</td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td>
         TBD
      </td>
   </tr>
</table>

###<a id="boshTestAvailabilityMetric"></a>BOSH Director Health Test Availability
<table>
   <tr><th colspan="2" style="text-align: center;"><br/> health.check.bosh.director.probe.available<br/><br/></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>
        PCF Healthwatch has up-to-date results for the  [BOSH Director Health Test](metrics.html#bosh-director), meaning that testing was recently available.
        <br/><br/>
        1 = available<br/>
        0 = not available, or timeout (10s)
        <br/><br/>
        An assessment of up-to-date results is made by looking for results within the configured run schedule plus timeout configured. For example, a test runner scheduled on 5 minute intervals with a 2 minute timeout, would need to show a test result within the last 7 minutes to succeed.
        <br/><br/>
        <strong>Use</strong>:  Indicates that Healthwatch is assessing the current state of health for the BOSH Director Health. If this continuous validation test fails to make up-to-date assessments, it is no longer a reliable warning mechanism of problems resulting in potential end-user impact.
        <br/><br/>
        <strong>Origin</strong>: Firehose<br/>
        <strong>Type</strong>: Gauge<br/>
        <strong>Frequency</strong>: 60s<br/>
      </td>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>Average over last 5 minutes</td>
   </tr>
   <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: &le; 0.6<br/>
      <strong>Red critical</strong>: &le; 0.4</td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td>
         TBD
      </td>
   </tr>
</table>

###<a id="opsmanTestAvailabilityMetric"></a>Ops Manager Health Test Availability
<table>
   <tr><th colspan="2" style="text-align: center;"><br/> health.check.OpsMan.probe.available<br/><br/></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>
        PCF Healthwatch has up-to-date results for the [OpsMan Health Test](metrics.html#opsman), meaning that testing was recently available.
        <br/><br/>
        1 = available<br/>
        0 = not available, or timeout (10s)
        <br/><br/>
        An assessment of up-to-date results is made by looking for results within the configured run schedule plus timeout configured. For example, a test runner scheduled on 5 minute intervals with a 2 minute timeout, would need to show a test result within the last 7 minutes to succeed.
        <br/><br/>
        <strong>Use</strong>: Indicates that Healthwatch is assessing the current state of health for the Ops Manager Health. If this continuous validation test fails to make up-to-date assessments, it is no longer a reliable warning mechanism of problems resulting in potential end-user impact.
        <br/><br/>
        <strong>Origin</strong>: Firehose<br/>
        <strong>Type</strong>: Gauge<br/>
        <strong>Frequency</strong>: 60s<br/>
      </td>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>Average over last 5 minutes</td>
   </tr>
   <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: &le; 0.6<br/>
      <strong>Red critical</strong>: &le; 0.4</td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td>
         TBD
      </td>
   </tr>
</table>

###<a id="superMetricsPublishedMetric"></a>Number of Healthwatch Super Metrics Published to Firehose
<table>
   <tr><th colspan="2" style="text-align: center;"><br/> health.check.OpsMan.probe.available<br/><br/></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>
        Number of Healthwatch assessment [metrics](metrics.html) published back to the firehose
        <br/><br/>
        <strong>Use</strong>: An unusual drop in assessment metrics published, without purposeful Operator change that impacts the number or frequency of assessments, can indicate that Healthwatch may be experiencing a computation or publication issue.
        <br/><br/>
        <strong>Origin</strong>: Firehose<br/>
        <strong>Type</strong>: Gauge<br/>
        <strong>Frequency</strong>: 60s<br/>
      </td>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>Average over last 5 minutes</td>
   </tr>
   <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: Dynamic<br/>
      <strong>Red critical</strong>: Dynamic</td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td>
         TBD
      </td>
   </tr>
</table>

##<a id="continuousValidationTests"></a>Number of Healthwatch Continuous Validation Tests Executed

###<a id="cliCommandHealthProbeMetric"></a>CLI Command Health
<table>
   <tr><th colspan="2" style="text-align: center;"><br/> health.check.cliCommand.probe.count<br/><br/></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>
        Number of Healthwatch [CLI Command Health](metrics.html#cli) probe assessments completed in the measured time interval.
        <br/><br/>
        <strong>Use</strong>: For alerting purposes, we suggest alerting on `health.check.cliCommand.probe.available` instead. This metric is most helpful for additional diagnostics or secondary alerting.
        <br/><br/>
        Primary indicator of concern with this metric would be an unexpected negative variance from the normal pattern of checks per test type. If an Operator has not taken action that would impact the number of checks being made, like scaling the test runner or changing the frequency of the run, then an unexpected variance from normal would likely indicate problems in the test runner functionality.
        <br/><br/>
        In the default install, these tests run every 5 minutes, across 2 runner apps.
        <br/><br/>
        <strong>Origin</strong>: Firehose<br/>
        <strong>Type</strong>: Gauge<br/>
        <strong>Frequency</strong>: 60s<br/>
      </td>
   </tr>
</table>

###<a id="opsmanagerHealthProbeMetric"></a>Ops Manager Health
<table>
   <tr><th colspan="2" style="text-align: center;"><br/> health.check.OpsMan.probe.count<br/><br/></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>
        Number of Healthwatch [OpsMan Health](metrics.html#opsman) probe assessments completed in the measured time interval.
        <br/><br/>
        <strong>Use</strong>: For alerting purposes, we suggest alerting on `health.check.OpsMan.probe.available` instead. This metric is most helpful for additional diagnostics or secondary alerting.
        <br/><br/>
        Primary indicator of concern with this metric would be an unexpected negative variance from the normal pattern of checks per test type. If an Operator has not taken action that would impact the number of checks being made, like scaling the test runner or changing the frequency of the run, then an unexpected variance from normal would likely indicate problems in the test runner functionality.
        <br/><br/>
        In the default install, these tests run every 1 minute, across 2 runner apps.
        <br/><br/>
        <strong>Origin</strong>: Firehose<br/>
        <strong>Type</strong>: Gauge<br/>
        <strong>Frequency</strong>: 60s<br/>
      </td>
   </tr>
</table>

###<a id="canaryHealthProbeMetric"></a>Canary App Health
<table>
   <tr><th colspan="2" style="text-align: center;"><br/> health.check.CanaryApp.probe.count<br/><br/></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>
        Number of Healthwatch [Canary App Health](metrics.html#canaryapp) probe assessments completed in the measured time interval.
        <br/><br/>
        <strong>Use</strong>: For alerting purposes, we suggest alerting on `health.check.CanaryApp.probe.available` instead. This metric is most helpful for additional diagnostics or secondary alerting.
        <br/><br/>
        Primary indicator of concern with this metric would be an unexpected negative variance from the normal pattern of checks per test type. If an Operator has not taken action that would impact the number of checks being made, like scaling the test runner or changing the frequency of the run, then an unexpected variance from normal would likely indicate problems in the test runner functionality.
        <br/><br/>
        In the default install, these tests run every 1 minutes, across 2 runner apps.
        <br/><br/>
        <strong>Origin</strong>: Firehose<br/>
        <strong>Type</strong>: Gauge<br/>
        <strong>Frequency</strong>: 60s<br/>
      </td>
   </tr>
</table>

###<a id="boshHealthProbeMetric"></a>BOSH Director Health
<table>
   <tr><th colspan="2" style="text-align: center;"><br/> health.check.bosh.director.probe.count<br/><br/></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>
        Number of Healthwatch [BOSH Director Health](metrics.html#bosh-director) probe assessments completed in the measured time interval.
        <br/><br/>
        <strong>Use</strong>: For alerting purposes, we suggest alerting on `health.check.bosh.director.probe.available` instead. This metric is most helpful for additional diagnostics or secondary alerting.
        <br/><br/>
        Primary indicator of concern with this metric would be an unexpected negative variance from the normal pattern of checks per test type. If an Operator has not taken action that would impact the number of checks being made, like scaling the test runner or changing the frequency of the run, then an unexpected variance from normal would likely indicate problems in the test runner functionality.
        <br/><br/>
        In the default install, these tests run every 10 minutes, across 1 runner app.
        <br/><br/>
        <strong>Origin</strong>: Firehose<br/>
        <strong>Type</strong>: Gauge<br/>
        <strong>Frequency</strong>: 60s<br/>
      </td>
   </tr>
</table>

##<a id="otherMetrics"></a>Other metrics available to operationally monitor PCF Healthwatch
###<a id="boshDeploymentProbeMetric"></a>BOSH Deployment Check Probe
<table>
   <tr><th colspan="2" style="text-align: center;"><br/> health.bosh.deployment.probe.count<br/><br/></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>
        Number of Healthwatch [BOSH Deployment Occurrence](metrics.html#bosh-deployment) probes completed in the measured time interval.
        <br/><br/>
        <strong>Use</strong>: Primary indicator of concern with this metric would be an unexpected negative variance from the normal pattern of checks per test type. If an Operator has not taken action that would impact the number of checks being made, like scaling the test runner or changing the frequency of the run, then an unexpected variance from normal would likely indicate problems in the test runner functionality.
        <br/><br/>
        Primary indicator of concern with this metric would be an unexpected negative variance from the normal pattern of checks per test type. If an Operator has not taken action that would impact the number of checks being made, like scaling the test runner or changing the frequency of the run, then an unexpected variance from normal would likely indicate problems in the test runner functionality.
        <br/><br/>
        In the default install, these tests run every 30 seconds, across 2 runner apps.
        <br/><br/>
        <strong>Origin</strong>: Firehose<br/>
        <strong>Type</strong>: Gauge<br/>
        <strong>Frequency</strong>: 60s<br/>
      </td>
   </tr>
</table>

