---
title: Installing Healthwatch
owner: Healthwatch
---

<strong><%= modified_date %></strong>

This topic describes how to install and configure Healthwatch.


## <a id='overview'></a> Overview

The Healthwatch tile monitors metrics across multiple foundations using exporter tiles from
each foundation.

To install and configure Healthwatch:

1. Install the Healthwatch tile in the Ops Manager Installation Dashboard. For more information,
see [Install the Healthwatch Tile](#install) below.

1. Configure the Healthwatch tile. For more information, see [Configure Healthwatch](#configure)
below.

1. Deploy the Healthwatch tile. For more information, see [Deploy Healthwatch](#apply-changes)
below.

After completing the procedures in this topic, continue to either [Installing Healthwatch Exporter
for TAS for VMs](installing-exporter-tas-vms.html) or [Installing Healthwatch Exporter for
TKGI](installing-exporter-pks.html) to install exporter tiles for the foundations you want
to monitor.


## <a id='install'></a> Install the Healthwatch Tile

There are two ways you can install the Healthwatch tile:

* Download the Healthwatch tile from VMware Tanzu Network. For more information, see [Install
Healthwatch from VMware Tanzu Network](#pivnet) below.

* Fetch the tile using Platform Automation. For more information, see [Install Healthwatch
with Platform Automation](#platform-automation) below.

### <a id='pivnet'></a> Install Healthwatch from VMware Tanzu Network

If you are manually configuring Healthwatch, you can install the Healthwatch tile from VMware
Tanzu Network.

To install the Healthwatch tile from VMware Tanzu Network:

1. Navigate to the [Healthwatch](https://network.pivotal.io/products/p-healthwatch/) page on
VMware Tanzu Network.

1. From the **Releases** dropdown, select the version you want to install.

1. Click **Healthwatch** to download the Healthwatch `.pivotal` file.

1. Navigate to the Ops Manager Installation Dashboard.

1. Under the **Import a Product** button, click the **+** icon next to the Healthwatch listing
to add the tile to your staging area. For more information, see [Adding and Deleting Products]
(https://docs.pivotal.io/ops-manager/install/add-delete.html) in the Ops Manager documentation.

### <a id='platform-automation'></a> Install Healthwatch with Platform Automation

If you want to use automation scripts to configure Healthwatch, you can use Platform Automation
to fetch the Healthwatch tile using a configuration file.

1. Create a configuration file for the `download-product` task of your automated pipeline.
The following configuration file fetches the Healthwatch tile from VMware Tanzu Network:

    ```
    ---
    pivnet-api-token: token
    pivnet-file-glob: "healthwatch-[^pas|pks].*pivotal"
    pivnet-product-slug: p-healthwatch
    product-version-regex: 2\.0.*
    ```
    For more information, see [Download](https://docs.pivotal.io/platform-automation/v5.0/how-to-guides/adding-a-product.html#download)
    in _Extending a Pipeline to Install a Product_ in the Platform Automation documentation.

1. Upload and stage the Healthwatch tile to the Ops Manager Installation Dashboard. For more
information, see [Upload and Stage](https://docs.pivotal.io/platform-automation/v5.0/how-to-guides/adding-a-product.html#upload-and-stage)
in _Extending a Pipeline to Install a Product_ in the Platform Automation documentation.

For more information about adding a product to the Ops Manager Installation Dashboard through
Platform Automation, see [Extending a Pipeline to Install a Product](https://docs.pivotal.io/platform-automation/v5.0/how-to-guides/adding-a-product.html)
in the Platform Automation documentation.


## <a id='configure'></a> Configure the Healthwatch Tile

This section describes how to configure the Healthwatch tile.

To configure the Healthwatch tile:

<div class='note'><strong>Notes:</strong>
  <ul>
    <li>If you want to quickly deploy the Healthwatch tile to ensure that it successfully deploys
      before you fully configure it, you only need to configure the <strong>Assign AZ and Networks</strong>
      pane.</li>
    <li>If you are using Healthwatch to monitor foundations that are running Tanzu Kubernetes
      Grid Integrated Edition (TKGI), VMware recommends configuring the <strong>TKGI Cluster
      Discovery Configuration</strong> pane.</li>
  </ul>
</div>

1. Click the **Healthwatch** tile.

1. Assign jobs to your Availability Zones (AZs) and networks. For more information, see [Assign
AZs and Networks](#az) below.

1. (Optional) Configure the **Prometheus Configuration** pane. For more information, see [(Optional)
Configure Prometheus](#prometheus) below.

1. (Optional) Configure the **Alertmanager Configuration** pane. For more information, see
[(Optional) Configure Alertmanager](#alertmanager) below.

1. (Optional) Configure the **Grafana Configuration** pane. For more information, see [(Optional)
Configure Grafana](#grafana) below.

1. (Optional) Configure the **Canary URL Configuration** pane. For more information, see [(Optional)
Configure Canary URL](#canary) below.

1. (Optional) Configure the **Remote Write Configuration** pane. For more information, see
[(Optional) Configure Remote Write](#remote-write) below.

1. (Optional) Configure the **TKGI Cluster Discovery Configuration** pane. For more information,
see [(Optional) Configure TKGI Discovery](#pks-cluster-discovery) below.

1. (Optional) Configure the **Errands** pane. For more information, see [(Optional) Configure
Errands](#errands) below.

1. (Optional) Configure the **Syslog** pane. For more information, see [(Optional) Configure
Syslog](#syslog) below.

1. (Optional) Configure the **Resource Config** pane. For more information, see [(Optional)
Configure Resources](#resource-config) below.

### <a id='az'></a> Assign AZs and Networks

In the **Assign AZ and Networks** pane, you assign jobs to your AZs and networks.

To configure the **Assign AZ and Networks** pane:

1. Select **Assign AZs and Networks**.

1. Under **Place singleton jobs in**, select the first AZ. Ops Manager runs any job with a
single instance in this AZ.

1. Under **Balance other jobs in**, select one or more other AZs. Ops Manager balances instances
of jobs with more than one instance across the AZs that you specify.

1. From the **Network** dropdown, select the runtime network that you created when configuring
the BOSH Director tile.

1. Click **Save**.

### <a id='prometheus'></a> (Optional) Configure Prometheus

In the **Prometheus Configuration** pane, you configure the Prometheus VM to scrape metrics
from the exporters in each foundation.

The **Prometheus Configuration** pane configures the `scrape_config` and `tls_config` sections
of the Prometheus configuration file. For more information, see [&#60;scrape_config>](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config) and [&#60;tls_config>](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#tls_config)
in _Configuration_ in the Prometheus documentation.

To configure the **Prometheus Configuration** pane:

1. Select **Prometheus Configuration**.

1. For **Scrape Interval**, specify the frequency at which you want the Prometheus VM to scrape
exporters for metrics. You can enter a value string that specifies `ns`, `us`, `Âµs`, `ms`,
`s`, `m`, or `h`. To scrape detailed metrics without consuming too much storage, VMware recommends
using the default value of `15s`, or 15 seconds.

1. (Optional) To configure the Prometheus VM to scrape metrics from exporters installed on
other foundations or from external apps or services, configure additional scrape jobs under
**Additional Scrape Config Jobs**. You can configure scrape jobs for any app or service that
exposes metrics using a Prometheus exposition format, such as Concourse CI. For more information
about Prometheus exposition formats, see [Exposition formats](https://github.com/prometheus/docs/blob/master/content/docs/instrumenting/exposition_formats.md)
in the Prometheus repository on GitHub.
  <p class="note"><strong>Note:</strong> The Prometheus VM automatically discovers and scrapes
    exporters installed on the same foundation as Healthwatch. You do not need to configure
    scrape jobs for these exporters.</p>
  1. Click **Add**.
  1. For **TSDB Scrape job**, provide the configuration YAML for the scrape job you want to
  configure. This job can use any of the properties defined by Prometheus except the `tls_config`
  property. Do not prefix the configuration YAML with a dash. For example:

        ```
        job_name: foundation-1
        metrics_path: /metrics
        scheme: https
        static_configs:
         - targets:
           - "1.2.3.4:9090"
           - "5.6.7.8:9090"
        ```
        For more information, see [&#60;scrape_config>](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config)
        in _Configuration_ in the Prometheus documentation.
        <div class="note warning"> <strong>Warning:</strong> For the <code>job_name</code> property,
          do not use the following job names:
          <ul>
            <li><code>Healthwatch-view-pas-exporter</code></li>
            <li><code>Healthwatch-view-pks-exporter</code></li>
            <li><code>tsdb</code></li>
            <li><code>grafana</code></li>
            <li><code>pks-master-kube-scheduler</code></li>
            <li><code>pks-master-kube-controller-manager</code></li>
          </ul>
        </div>
  1. (Optional) To configure TLS communication between the Prometheus VM and your external
  app or service:
      1. For **TLS Config Certificate Authority**, provide a certificate authority (CA) that
      signs the certificates you provide in the **TLS Config Certificate and Private Key**
      field below. This CA appears as the `ca_file` property in the `tls_config` section of
      the Prometheus configuration file.
      1. For **TLS Config Certificate and Private Key**, provide at least one certificate and
      private key to enable TLS communication between the Prometheus VM and your external app
      or service. These certificates and private keys appear as the `cert_file` and `key_file`
      properties in the `tls_config` section of the Prometheus configuration file.
      1. For **TLS Config Server Name**, enter the name of the server that facilitates TLS
      communication between the Prometheus VM and your external app or service. This server
      name appears as the `server_name` property in the `tls_config` section of the Prometheus
      configuration file.

1. (Optional) For **Chunk Size (Disk) MB**, enter in MB the size that you want to specify for
chunks of free disk. The default value is `6144`. Healthwatch uses this free disk chunk size
to calculate the available disk chunks super value metric (SVM). If you rely on Pivotal Healthwatch
v1.8 or earlier for any metrics, the Pivotal Healthwatch integration then uses this SVM to
calculate the `Diego_AvailableFreeChunksDisk` metric. The Pivotal Healthwatch integration sends
the `Diego_AvailableFreeChunksDisk` metric back into Loggregator so third-party nozzles can
send it to external destinations, such as a remote server or external aggregatation service.

1. (Optional) For **Chunk Size (Memory) MB**, enter in MB the size that you want to specify
for chunks of free memory. The default value is `4096`. Healthwatch uses this free memory chunk
size to calculate the available memory chunks super value metric (SVM). If you rely on Pivotal
Healthwatch v1.8 or earlier for any metrics, the Pivotal Healthwatch integration then uses
this SVM to calculate the `Diego_AvailableFreeChunksMemory` metric. The Pivotal Healthwatch
integration sends the `Diego_AvailableFreeChunksMemory` metric back into Loggregator so third-party
nozzles can send it to external destinations, such as a remote server or external aggregatation
service.

1. Click **Save**.

### <a id='alertmanager'></a> (Optional) Configure Alertmanager

In the **Alertmanager Configuration** pane, you configure alerting for Healthwatch.

The **Alertmanager Configuration** pane configures the Alertmanager configuration file. For
more information, see [Configuration](https://prometheus.io/docs/alerting/latest/configuration/)
in the Prometheus documentation.

To configure the **Alertmanager Configuration** pane, see [Alerting](alerting.html).

### <a id='grafana'></a> (Optional) Configure Grafana

In the **Grafana Configuration** pane, you configure how users access and authenticate with
the Grafana UI, as well as which dashboards appear in the Grafana UI.

To configure the **Grafana Configuration** pane:

1. Select **Grafana Configuration**.

1. If you configured your Grafana VM to allow access to the Grafana UI from a URL outside your
BOSH network, enter the full URL in **Root URL for Grafana**. You must provide this URL to
enable generic OAuth or UAA to redirect users to the Grafana UI. The Alertmanager also uses
this URL to generate links to the Grafana UI in alert messages.
<br>
<br>
This URL must be configured in your DNS provider to point to the public IP address of either
a single Grafana VM or the load balancer in front of multiple Grafana VMs. You can obtain this
IP address to configure **Root URL for Grafana** after you deploy the Healthwatch tile for
the first time. Your Grafana VMs listen on either port `443` or `80`, depending on whether
you provide an SSL certificate in the **Enable HTTPS by providing certificates** field below.
For more information about configuring DNS for the Grafana VM, see [Configuring DNS for the
Grafana VM](common-configurations/dns-configuration.html).

1. Under **Enable HTTP(s) Proxy Settings for Grafana**, choose whether to enable or disable
the Grafana VM to make HTTP and HTTPS proxy requests:
  1. To disable HTTP and HTTPS proxy requests, select **Disabled**. HTTP and HTTPS proxy settings
  are disabled by default.
  1. To configure proxy settings for the Grafana VM:
      1. Select **Enabled**.
      1. For **HTTP Proxy for Grafana**, enter your HTTP proxy server URL. The Grafana VM uses
      this URL as the proxy URL for all HTTP and HTTPS requests except those from hosts you
      configure in the **HTTPS Proxy for Grafana** and **No Proxy for Grafana** fields below.
      1. For **HTTPS Proxy for Grafana**, enter your HTTPS proxy server URL. The Grafana VM
      uses this URL as the proxy URL for all HTTPS requests except those from hosts you configure
      in the **No Proxy for Grafana** field below.
      1. For **No Proxy for Grafana**, enter a comma-separated list of the hosts you want to
      exclude from proxying. VMware recommends including `*.bosh` and the range of your internal
      network IP addresses so the Grafana VM can still access the Prometheus VM without going
      though the proxy. For example, `*.bosh,10.0.0.0/8,*.example.com` allows the Grafana VM
      to access all BOSH DNS addresses and all internal network IP addresses containing `10.0.0.0/8`
      or `*.example.com` directly, without going though the proxy.
      <p class="note"><strong>Note:</strong> You only need to configure proxy settings if you
        are deploying Healthwatch in an air-gapped environment and want to configure alert
        channels to external addresses, such as the external Slack webhook.</p>

1. (Optional) For **Static IPs for the Grafana VM(s)**, enter a comma-separated list of valid
static IP addresses that you want to reserve for the Grafana VM. These IP addresses must not
be within the reserved IP ranges you configured in the BOSH Director tile.

1. (Optional) To prevent users from logging in to Grafana UI with basic authentication, including
admin users, clear the **Enable Grafana Login Form** checkbox. This checkbox is selected by
default.

1. (Optional) If you have the Monitoring Indicator Protocol tile installed on your foundation,
enable the **Use Indicator Protocol Generated Dashboards** checkbox to enable the Grafana VM
to generate dashboards in the Grafana UI based on indicator documents the Monitoring Indicator
Protocol tile generates. This checkbox is disabled by default. For more information about Monitoring
Indicator Protocol, see [Indicator Protocol Dashboard (Beta)](indicator-protocol.html) and
the [Monitoring Indicator Protocol](https://github.com/pivotal/monitoring-indicator-protocol)
repository on GitHub.

1. Under **Discover Product Dashboards**, select how you want the Grafana VM to discover the
runtimes in your foundations.
    * **Dynamic:** The Grafana VM creates a dashboard in the Grafana UI for the versions of
    VMware Tanzu Application Service for VMs (TAS for VMs) or TKGI that are currently installed
    on your foundation. This option is selected by default.
    * **Manual:** The Grafana VM creates a dashboard in the Grafana UI for the versions of
    TAS for VMs or TKGI you specify in **TAS Version to Monitor** and **TKGI Version to Monitor**.
    * **Disabled:** The Grafana VM does not discover or create dashboards in the Grafana UI
    for TAS for VMs or TKGI.

1. (Optional) If you want Grafana to create a dashboard for RabbitMQ, select the **Enable RabbitMQ
dashboards** checkbox. This checkbox is disabled by default.

1. (Optional) To enable HTTPS for the Grafana VM, provide one or more SSL certificates in **Enable
HTTPS by providing certificates**.

1. (Optional) If you provided one or more SSL certificates in the previous step, provide a
CA for those SSL certificates in **CA for SSL certificates**.

1. (Optional) To configure an additional cipher suite for TLS connections to the Grafana VM,
enter a comma-separated list of ciphers in **Additional Cipher Suite Support**. For a list
of supported cipher suites, see [cipher_suites.go](https://github.com/golang/go/blob/c847589ad06a1acfcceaac7b230c0d5a826caab8/src/crypto/tls/cipher_suites.go#L500-L522) in the Go repository on GitHub.

1. Under **Select an authentication mechanism for Grafana**, select the user authentication
method you want the Grafana VM to use:
    * **Basic:** Enables basic authentication.
    * **Generic OAuth:** Enables OAuth authentication for any supported authentication provider,
    such as GitHub or Okta. For more information, see [Configuring Generic OAuth](common-configurations/grafana-authentication.html#configuring-generic-oauth)
    in _Grafana Authentication_ and [User Authentication Overview](https://grafana.com/docs/grafana/latest/auth/overview/)
    in the Grafana documentation.
    * **UAA:** Enables UAA authentication. For more information, see [Configuring UAA OAuth]
    (common-configurations/grafana-authentication.html#configuring-uaa-oauth) in _Grafana Authentication_.
    * **LDAP:** Enables LDAP authentication. For more information, see [LDAP Authentication]
    (https://grafana.com/docs/grafana/latest/auth/ldap/) in the Grafana documentation.

1. Under **Enable SMTP for Grafana Alerts**, choose whether to enable or disable email alerts
from the Grafana UI.
  * To diable email alerts, select **Disabled**. Email alerts are disabled by default.
  * To enable email alerts:
      1. Select **Enabled**.
      1. For **Host Name**, enter the host name of your SMTP server.
      1. For **Port**, enter the port of your SMTP server.
      1. For **Username**, enter your SMTP authentication username.
      1. For **Password**, enter your SMTP authentication password.
      1. (Optional) To enable the Grafana VM to skip SSL validation when communicating with
      your SMTP server over TLS, enable the **Skip SSL Verification** checkbox.
      1. For **From Address**, enter the sender email address that appears on outgoing email
      alerts.
      1. For **From Name**, enter the sender name that appears on outgoing email alerts.
      1. For **EHLO Identity**, enter a name for the client identity that your SMTP server
      uses when sending EHLO commands.
      1. For **TLS Credentials**, enter a certificate and private key to enable the Grafana
      VM to communicate with your SMTP server over TLS.
      <br>
      <br>
    For more information, see [&#91;smtp&#93;](https://grafana.com/docs/installation/configuration/#smtp)
    in the Grafana documentation.

1. Click **Save**.

### <a id='canary'></a> (Optional) Configure Canary URLs

In the **Canary URL Configuration** pane, you configure target URLs to which the Blackbox Exporter
in the Prometheus VM sends canary tests. Testing a canary target URL allow you to gauge the
overall health and accessibility of an app, runtime, or deployment.

The **Canary URL Configuration** pane configures the Blackbox Exporter in the Prometheus VM.
For more information, see the [Blackbox exporter](https://github.com/prometheus/blackbox_exporter)
repository on GitHub.

To configure the **Canary URL Configuration** pane:

1. Select **Canary URL Configuration**.

1. For **Exporter Port**, specify the port that the Blackbox Exporter exposes to the Prometheus
VM. The default port is `9115`. You do not need to specify a different port unless port `9115`
is already in use on the Prometheus VM.

1. (Optional) For **Ops Manager URL**, enter the fully-qualified domain name (FQDN) of your
Ops Manager deployment. This creates a canary target URL that allows the Blackbox Exporter
to test whether the Ops Manager Installation Dashboard is accessible. The results from these
canary tests appear in the **Ops Manager Health** dashboard in the Grafana UI.
  <p class="note"><strong>Note:</strong> If you have SAML authentication enabled for Ops Manager,
    enter <code>https<span>:</span>//OPS-MANAGER-FQDN/api/v0/info</code>, where <code>OPS-MANAGER-FQDN</code>
    is the FQDN of your your Ops Manager deployment. Otherwise, the canary test fails.</p>

1. (Optional) Under **Target URLs**, you can configure canary target URLs. The Prometheus VM
runs continuous canary tests to these URLs and records the results. To configure canary target
URLs:
  1. Click **Add**.
  1. For **HTTP(S) URL**, enter the URL to which you want the Prometheus VM to send canary
  tests. VMware recommends including `apps.sys.FOUNDATION-URL` if you have TAS for VMs installed,
  or `api.pks.FOUNDATION-URL:8443` if you have TKGI installed, where `FOUNDATION-URL` is the
  root URL of your foundation.
  <p class='note'><strong>Note:</strong> The Prometheus VM automatically creates scrape jobs
    for these URLs. You do not need to create additional scrape jobs for them in the <strong>Prometheus
    Configuration</strong> pane.</p>

1. Click **Save**.

### <a id='remote-write'></a> (Optional) Configure Remote Write

In the **Remote Write Configuration** pane, you can configure the Prometheus VM to write to
remote storage, in addition to its local time series database (TSDB). Healthwatch stores monitoring
data for six weeks before deleting it. Configuring remote write enables Healthwatch to store
data that is older than six weeks in a remote database or storage endpoint. For a list of compatible
remote databases and storage endpoints, see [Remote Endpoints and Storage](https://prometheus.io/docs/operating/integrations/#remote-endpoints-and-storage)
in _Integrations_ in the Prometheus documentation.

The **Remote Write Configuration** pane configures the `remote_write` section of the Prometheus
configuration file. For more information, see [&#60;remote_write>](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#remote_write)
in _Configuration_ in the Prometheus documentation.

To configure the **Remote Write Configuration** pane:

1. Select **Remote Write Configuration**.

1. Click **Add**.

1. For **URL**, enter the URL for your remote storage endpoint. For example, `https://REMOTE-STORAGE-FQDN`,
where `REMOTE-STORAGE-FQDN` is the FQDN of your remote storage endpoint.

1. (Optional) In **Remote Timeout (seconds)**, enter in seconds the amount of time that the
Prometheus VM must try to make a request to the remote storage endpoint before the request
fails.

1. (Optional) To enable the Prometheus VM to write to your storage endpoint using basic authentication:
  1. For **Basic Auth Username**, enter the username that the Prometheus VM uses to log in
  to your remote storage endpoint.
  1. For **Basic Auth Password**, enter the password that the Prometheus VM uses to log in
  to your remote storage endpoint.

1. (Optional) To configure TLS communication between the Prometheus VM and your remote storage
endpoint:
  1. For **TLS Config Certificate Authority**, provide a CA that signs the certificates you
  provide in the **TLS Config Certificate and Private Key** field below. This CA appears as
  the `ca_file` property in the `tls_config` section of the `remote_write` configuration.
  1. For **TLS Config Certificate and Private Key**, provide at least one certificate and private
  key to enable TLS communication between the Prometheus VM and your remote storage endpoint.
  These certificates and private keys appear as the `cert_file` and `key_file` properties in
  the `tls_config` section of the `remote_write` configuration.
  1. For **TLS Config Server Name**, enter the name of the server that facilitates TLS communication
  between the Prometheus VM and your remote storage endpoint. This server name appears as the
  `server_name` property in the `tls_config` section of the `remote_write` configuration.
  1. If the certificate you provided in the **TLS Config Certificate and Private Key** field
  is signed by a self-signed CA or a certificate that is signed by a self-signed CA, enable
  the **TLS Config Skip SSL Validation** checkbox to skip SSL validation during TLS handshakes.

1. (Optional) To enable the Prometheus VM to make HTTP or HTTPS proxy requests to the remote
storage endpoint, enter a proxy URL in **Proxy URL**.

1. (Optional) You can configure more granular settings for writing to your remote storage endpoint
by specifying additional parameters for the shards containing in-memory queues that read from
the write-ahead log (WAL) in the Prometheus VM. The following fields configure the `queue_config`
section of the remote write configuration:
  1. For **Queue Capacity**, enter how many samples the remote storage endpoint can queue in
  memory per shard before the Prometheus VM blocks the queue from reading from the WAL. This
  number appears as the `capacity` property in the `queue_config` section of the remote write
  configuration.
  1. For **Maximum Number of Shards**, enter the maximum number of shards the Prometheus VM
  can use for each remote write queue. This number appears as the `max_shards` property in
  the `queue_config` section of the remote write configuration.
  1. For **Minimum Number of Shards**, enter the minimum number of shards the Prometheus VM
  can use for each remote write queue. This number is also the number of shards the Prometheus
  VM uses when remote write begins after each deployment of the Healthwatch tile. This number
  appears as the `min_shards` property in the `queue_config` section of the remote write configuration.
  1. For **Maximum Number of Samples Per Send**, enter the maximum number of samples the Prometheus
  VM can send to a shard at a time. This number appears as the `max_samples_per_send` property
  in the `queue_config` section of the remote write configuration.
  1. For **Batch Send Deadline (seconds)**, enter in seconds the maximum amount of time the
  Prometheus VM can wait before sending a batch of samples to a shard, whether that shard has
  reached the limit configured in **Maximum Number of Samples Per Send** or not. This number
  appears as the `batch_send_deadline` property in the `queue_config` section of the remote
  write configuration.
  1. For **Minimum Backoff Time (milliseconds)**, enter in milliseconds the minimum amount
  of time the Prometheus VM can wait before retrying a failed request to the remote storage
  endpoint. This number appears as the `min_backoff` property in the `queue_config` section
  of the remote write configuration.
  1. For **Maximum Retry Delay (milliseconds)**, enter in milliseconds the maximum amount of
  time the Prometheus VM can wait before retrying a failed request to the remote storage endpoint.
  This number appears as the `max_backoff` property in the `queue_config` section of the remote
  write configuration.
  <br>
  <br>
  For more information about configuring these queue parameters, see [Remote Write Tuning]
  (https://prometheus.io/docs/practices/remote_write/) in the Prometheus documentation.

1. Click **Save**.

### <a id='tkgi-cluster-discovery'></a> (Optional) Configure TKGI Cluster Discovery

In the **TKGI Cluster Discovery Configuration** pane, you can enable the Prometheus VM to detect
clusters created by the TKGI API and create scrape jobs for them. You only need to configure
this pane if you have foundations with TKGI installed.

To configure the **TKGI Cluster Discovery Configuration** pane:

1. Select **TKGI Cluster Discovery Configuration**.

1. Under **Enable TKGI Cluster Discovery**, select one of the following options:
    * **Disabled:** This option disables TKGI cluster discovery. TKGI cluster discovery is
    disabled by default.
    * **Enabled:** This option enables TKGI cluster discovery and reveals the configuration
    fields described in the steps below.

1. (Optional) For **Scrape Port**, enter a port where the Healthwatch tile exposes an endpoint
from which the Prometheus VM scrapes metrics detailing the health of the TKGI cluster discovery
process. These metrics appear in the **Healthwatch - Exporter Troubleshooting** dashboard in
the Healthwatch tile.

1. For **TKGI API Address**, enter the TKGI API domain you configured in the **API Hostname
(FQDN)** field in the **TKGI API** pane of the TKGI tile. For example, `api.tkgi.example.com`.
For more information, see [TKGI API Authentication](https://docs.pivotal.io/tkgi/api-auth.html)
in the TKGI documentation.

1. For **TKGI UAA Client**, enter one of the following options for the TKGI UAA client username:
    * Enter the TKGI management admin client username `admin`.
    * Create a separate UAA client with access to the TKGI API and enter the client username
    you specify. For more information, see [TKGI API Authentication](https://docs.pivotal.io/tkgi/api-auth.html)
    in the TKGI documentation.

1. For **TKGI UAA Client Secret**, enter one of the following options for the TKGI UAA client
secret:
    * Enter the TKGI management admin client secret:
      1. Return to the Ops Manager Installation Dashboard.
      1. Click the **Tanzu Kubernetes Grid Integrated Edition** tile.
      1. Select the **Credentials** tab.
      1. Click **Link to Credential** next to **Pks Uaa Management Admin Client**.
      1. Record the value of `secret`.
      1. In the **Healthwatch** tile, enter the client secret you recorded in the previous
      step into **TKGI UAA Client Secret**.
    * Create a separate UAA client with access to the TKGI API and enter the client secret
    you specify. For more information, see [Grant Tanzu Kubernetes Grid Integrated Edition
    Access to a Client](https://docs.pivotal.io/tkgi/manage-users.html#uaa-client) in _Managing
    Tanzu Kubernetes Grid Integrated Edition Users with UAA_ in the TKGI documentation.

1. If you configured UAA as the OIDC provider for TKGI in the **UAA** pane of the TKGI tile,
enter the TKGI UAA admin password in **TKGI UAA Admin Password**. Otherwise, do not configure
this field. For more information, see [Grant Tanzu Kubernetes Grid Integrated Edition Access
to a Client](https://docs.pivotal.io/tkgi/manage-users.html#uaa-client) in _Managing Tanzu
Kubernetes Grid Integrated Edition Users with UAA_ in the TKGI documentation.

1. (Optional) For **Test Frequency in Seconds**, enter in seconds how frequently you want the
TKGI Service Level Indicator (SLI) test to run. The TKGI SLI test monitors the health of the
TKGI API by logging into the TKGI API server, listing all TKGI clusters, and logging out of
the TKGI API server.

1. (Optional) To configure TLS communication between Healthwatch and the TKGI API, configure
one of the following options:
    * To configure Healthwatch to use a self-signed CA or a certificate that is signed by a
    self-signed CA when communicating with the TKGI API over TLS, provide the CA in **TKGI
    API Certificate Authority**. If you provide a self-signed CA, it must be the same CA that
    signs the certificate in the TKGI API.
    * If you do not provide a self-signed CA or a certificate that is signed by a self-signed
    CA in the **TKGI API Certificate Authority** field, you can enable the **TKGI API Skip
    SSL Validation** checkbox to enable Healthwatch to skip SSL validation when connecting
    to the TKGI API. VMware does not recommend skipping SSL validation in a production environment.

1. Click **Save**.

1. Return to the Ops Manager Installation Dashboard.

1. Click the **Tanzu Kubernetes Grid Integrated Edition** tile.

1. Select **Host Monitoring**.

1. Under **Enable Telegraf Outputs?**, select **Yes**.

1. Enable the **Include etcd metrics** checkbox to send etcd server and debugging metrics to
Healthwatch.

1. For **Setup Telegraf Outputs**, provide the following configuration file:

    ```toml
    [[outputs.prometheus_client]]
          listen = ":10200"
    ```
    You must use `10200` as the listening port to enable the Prometheus VM to scrape Telegraf
    metrics. For more information about creating a configuration file in TKGI, see [Create
    a Configuration File](https://docs.pivotal.io/tkgi/monitor-etcd.html#toml) in _Configuring
    Telegraf in TKGI_ in the TKGI documentation.

1. Click **Save**.

1. For each plan you want to monitor:
  1. Select the plan you want to monitor. For example, **Plan 2**.
  1. For **(Optional) Add-ons - Use with caution**, enter the following YAML snippet to configure
  the roles required to enable the Prometheus VM to scrape metrics from your TKGI clusters:

      <%= partial 'snippets/cluster_scraping_config.md' %>
      If **(Optional) Add-ons - Use with caution** already contains other API resource definitions,
      append the above YAML snippet to the end of the existing resource definitions, followed
      by a newline character.

1. Click **Save**.

1. Select **Errands**.

1. Ensure that the **Upgrade all clusters** errand is enabled. Running this errand propogates
the add-on resources provided in each plan to on-demand clusters.

1. Click **Save**.

### <a id='errands'></a> (Optional) Configure Errands

Errands are scripts that Ops Manager runs automatically when it installs or uninstalls a product,
such as a new version of Healthwatch. There are two types of errands: _post-deploy errands_
run after the product is installed, and _pre-delete errands_ run before the product is uninstalled.
However, there are no pre-delete errands for Healthwatch.

By default, Ops Manager always runs all errands.

In the **Errands** pane, you can select **On** to always run an errand or **Off** to never
run it.

For more information about how Ops Manager manages errands, see [Managing Errands in Ops Manager]
(https://docs.pivotal.io/ops-manager/install/managing_errands.html) in the Ops Manager documentation.

To configure the **Errands** pane:

1. Select **Errands**.

1. Choose whether to always run or never run the following errands:
    * **Smoke Test Errand:** Verifies that the Grafana and Prometheus VMs are running.
    * **Update Grafana Admin Password:** Updates the admin password for the Grafana UI.

1. Click **Save**.

### <a id='syslog'></a> (Optional) Configure Syslog

In the **Syslog** pane, you can configure system logging in Healthwatch to forward log messages
from Healthwatch component VMs to an external destination for troubleshooting, such as a remote
server or external syslog aggregation service.

To configure the **Syslog** pane:

1. Select **Syslog**.

1. Under **Do you want to configure Syslog forwarding?**, select one of the following options:
    * **No, do not forward Syslog:** Disables syslog forwarding.
    * **Yes:** Enables syslog forwarding and allows you to edit the configuration fields described
    below.

1. For **Address**, enter the IP address or DNS domain name of your external destination.

1. For **Port**, enter a port on which your external destination listens.

1. For **Transport Protocol**, select **TCP** or **UDP** from the dropdown. This determines
which transport protocol Healthwatch uses to forward system logs to your external destination.

1. (Optional) To transmit logs over TLS:
  1. Select the **Enable TLS** checkbox. This checkbox is disabled by default.
  1. For **Permitted Peer**, enter either the name or SHA1 fingerprint of the remote peer.
  1. For **SSL Certificate**, enter the SSL certificate for your external destination.

1. (Optional) For **Queue Size**, specify the number of log messages Healthwatch can hold in
a buffer at a time before sending them to your external destination. The default value is `100000`.

1. (Optional) To forward debug logs to your external destination, enable the **Forward Debug
Logs** checkbox. This checkbox is disabled by default.

1. (Optional) To specify a custom syslog rule, enter it in **Custom rsyslog configuration**
in RainerScript syntax. For more information about custom syslog rules, see [Customizing Platform
Log Forwarding](https://docs.pivotal.io/application-service/operating/custom-syslog-rules.html)
in the TAS for VMs documentation. For more information about RainerScript syntax, see the [rsyslog]
(https://www.rsyslog.com/doc/v8-stable/rainerscript/index.html) documentation.

1. Click **Save Syslog Settings**.

### <a id='resource-config'></a> (Optional) Configure Resources

In the **Resource Config** pane, you can scale Healthwatch component VMs up or down according
to the needs of your deployment, as well as associate load balancers with a group of VMs. For
example, you can scale the persistent disk size of the Prometheus VM to enable longer data
retention.

To configure the **Resource Config** pane:

1. Select **Resource Config**.

1. (Optional) To scale a job, select an option from the dropdown for the resource you want
to modify:
    * **Instances:** Configures the number of instances each job has.
    * **VM Type:** Configures the type of VM used in each instance.
    * **Persistent Disk Type:** Configures the amount of persistent disk space to allocate
    to the job.

1. (Optional) To add a load balancer to a job:
  1. Click the icon next to the job name.
  1. For **Load Balancers**, enter the name of your load balancer.
  1. Ensure that the **Internet Connected** checkbox is disabled. Enabling this checkbox gives
  VMs a public IP address that enables outbound Internet access.

1. Click **Save**.


## <a id='om'></a> Configuring and Deploying Healthwatch Using the om CLI

If you want to configure and deploy the Healthwatch tile using an automation script, you can
do so using the `om` CLI. Below is an example of an automation script that configures and deploys
Healthwatch:

<%= partial 'snippets/scrape_config.md' %>

VMware recommends configuring and deploying Healthwatch manually the first time, so you can
see which properties in the automation script correlate to the configuration settings present
in the Healthwatch UI before you modify and re-deploy the tile with the `om` CLI.

For more information about configuring and deploying tiles with the `om` CLI, see the [om]
(https://github.com/pivotal-cf/om) repository on GitHub.


## <a id='apply-changes'></a> Deploy Healthwatch

To complete your installation of the Healthwatch tile:

1. Return to the Ops Manager Installation Dashboard.

1. Click **Review Pending Changes**.

1. Click **Apply Changes**.

For more information, see [Reviewing Pending Product Changes](https://docs.pivotal.io/ops-manager/install/review-pending-changes.html)
in the Ops Manager documentation.


## <a id='next-steps'></a> Next Steps

After you have successfully installed the Healthwatch tile, continue to one of the following
topics to install exporter tiles on the foundations you want to monitor:

* If you have TAS for VMs installed on a foundation you want to monitor, see [Installing Healthwatch
Exporter for TAS for VMs](installing-exporter-tas-vms.html).

* If you have TKGI installed on a foundation you want to monitor, see [Installing Healthwatch
Exporter for TKGI](installing-exporter-pks.html).
