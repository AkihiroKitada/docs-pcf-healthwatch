---
title: Installing Healthwatch
owner: Healthwatch
---

This topic describes how to install and configure Healthwatch for Pivotal Cloud Foundry (PCF). 

## Install Healthwatch

1. Download the product file from [Healthwatch](https://network.pivotal.io/products/p-healthwatch/).

1. Navigate to the Ops Manager Installation Dashboard and click **Import a Product** to upload the product file.

1. Under the **Import a Product** button, click **+** next to the version number of Healthwatch.
This adds the tile to your staging area.

<p class="note"><strong>Note:</strong> Installing using Platform Automation </p>

The following config file for the `download-product` task will work to fetch the **Healthwatch** tile (without getting the exporters):
```yaml
---
pivnet-api-token: token
pivnet-file-glob: "Healthwatch-view-[^pks|pas]*.pivotal"
pivnet-product-slug: Healthwatch_view_pcf
product-version-regex: "0.4.*"
```

##  Configure the Healthwatch Tile

To configure the Healthwatch tile, perform the following steps:

1. Click the **Healthwatch** tile on the Ops Manager Installation Dashboard.
1. Navigate to **Assign AZs and Networks** and do the following:
    1. Select an Availability Zone (AZ) for placing singleton jobs.
    1. Select one or more AZs for balancing other jobs.
    1. Select **Network** for installing Healthwatch.
    1. Click **Save**.

1. Navigate to **Time series database(TSDB) Configuration** and do the following:
    1. Optional: Modify the **Scrape Interval** as desired. This controls the frequency at which the TSDB scrapes its targets for metrics.
    1. Optional: Add additional jobs to the `Additional Scrape Config Jobs`.  This entry section allows you to add jobs to the TSDB.yml configuration
       file.
       * The first field, **TSDB Scrape job**  takes a YAML formatted job in the format specified
       by [TSDB](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config)
](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config) . This job can use any of the properties
       defined by Prometheus itself except for the`tls_config` property.  The entries for that can be filled in below. Note that this box takes a single
       scrape job.  The job should not be prepended with the YAML array *-*.
       * Optional: **TLS Config Certificate Authority** This field takes a CA Certificate that will end up in the **ca_file** property of the **tls_config**.
       * Optional: **TLS Config Certificate and Private Key** This field takes a Certificate and Private key.  These properties will end up in the **cert_file**
       and **key_file** properties of the **tls_config**
       * Optional: **TLS Config Server Name** This field takes a string server name that will end up in the **server_name** field of the **tls_config**
   1. If using `om` to configure the tile, you will want a configuration like the following:
```yaml
{% include "product-config.yml" %}
```

1. Navigate to **UI Configuration** and do the following:
    1. Optional: If you configured Grafana to be accessible from outside the BOSH network, you may want to specify the
       `Root URL For the UI`. This should be set to the full URL used to access Grafana from a web browser.
       This is important if you use Google or GitHub OAuth authentication (for the callback URL to be correct).
    1. Optional: Enter a list of **Static IPs for the UI VM(s)**.  These IPs should be comma separated and be 
       valid static IPs from the IAAS for the network on which the tile is installed.
    1. Optional: **Enable UI Login Form**  Disabling this option will prevent anyone, including the admin user, from logging in with Basic Authentication
    1. Optional: **Select an authentication mechanism for the UI**.  Options include Basic, Generic OAuth(UAA, Github, etc) or LDAP. See [Grafana Authentication](../common-configurations/grafana-authentication.html) for detailed configurations.
    1. Optional: **Enable SMTP for Grafana Alerts** Enabling this option will allow Grafana to send emails for alerts that are configured through the dashboard.
       For individual field references, see [Grafana SMTP Configuration](https://grafana.com/docs/installation/configuration/#smtp).
1. Navigate to **Blackbox Exporter Configuration** and do the following:
    1. Optional: Enter **Blackbox Exporter Port** for the process to listen on.  It is only necessary to change from the default port if
       you have a port conflict on the TSDB VM.  The port is defaulted to `9115`.
    1. Optional: Enter a list of **Blackbox Exporter Targets**.  [The Blackbox Exporter](https://github.com/prometheus/blackbox_exporter) will run continuous probe tests
       on the URLs and record the results in the TSDB.  These probe tests are useful for generating SLI type metrics. There are no additional scrape configuration jobs necessary
       for the URLs entered here. They will be automatically added to the TSDB scrape jobs.

<p class="note"><strong>Note:</strong> Installing using Platform Automation </p>
When installing **Healthwatch PAS Exporter** or **Healthwatch PKS Exporter** on the same foundation as
**Healthwatch**, there is no need to manually configure a scrape job for them. These exporters are automatically discovered and scraped.
This section is for adding additional custom scrape jobs or to scrape exporters installed on different foundations.

<p class="note warning"><strong>Warning: </strong>Job Names</p>

When configuring custom scrape jobs, it is recommended that you avoid using the following job names:

      * `Healthwatch-view-pas-exporter`
      * `Healthwatch-view-pks-exporter`
      * `tsdb`
      * `ui`
      * `pks-master-kube-scheduler`
      * `pks-master-kube-controller-manager`

1. Return to the Ops Manager Installation Dashboard and click **Apply Changes**.

1. In order to access Grafana within **Healthwatch** an external IP address for the `ui` VM is needed along with allowing traffic to the VM on port 3000. A native IaaS load balancer can also be used. 

##  (Optional) Backing Up Healthwatch

Healthwatch supports [Bosh Backup and Restore](https://docs.pivotal.io/pivotalcf/2-4/customizing/backup-restore/index.html) functionality.
We recommend running nightly backups via a Concourse pipeline job like the one below:

```yaml
---
resource_types:
  - name: gcs-resource
    type: docker-image
    source:
      repository: frodenas/gcs-resource

  - name: pivnet
    type: docker-image
    source:
      repository: pivotalcf/pivnet-resource
      tag: latest-final

resources:
  - name: bbr-release
    type: pivnet
    source:
      api_token: ((pivnet-refresh-token))
      product_slug: p-bosh-backup-and-restore

  - name: bbr-pipeline-tasks-repo
    type: git
    source:
      uri: https://github.com/pivotal-cf/bbr-pcf-pipeline-tasks.git
      branch: master
      tag_filter: v1.0.0

  - name: bbr-docker
    type: docker-image
    source:
      repository: cloudfoundrylondon/bbr-pipeline
      tag: final

  - name: nightly
    type: time
    source:
      interval: 24h
      start: 22:00
      stop: 00:00
      location: America/Denver

  - name: rv-backup-bucket
    type: gcs-resource
    source:
      bucket: ((gcs-bucket-name))
      json_key: ((service-account-json))
      regexp: ((gcs-directory))/rv-backup-(.*).tar

jobs:
  - name: trigger-backup
    plan:
      - put: nightly

  - name: nightly-backup
    plan:
      - aggregate:
        - get: nightly
          trigger: true
        - get: bbr-docker
        - get: bbr-release
        - get: bbr-pipeline-tasks-repo
      - task: extract-binary
        file: bbr-pipeline-tasks-repo/tasks/extract-bbr-binary/task.yml
      - task: run-backup
        image: bbr-docker
        config:
          platform: linux

          inputs:
            - name: binary
            - name: bbr-pipeline-tasks-repo

          outputs:
            - name: rv-backup-artifact

          params:
            OPSMAN_URL: ((opsman-url))
            OPSMAN_USERNAME: ((opsman-user.username))
            OPSMAN_PASSWORD: ((opsman-user.password))
            OPSMAN_PRIVATE_KEY: ((ops-man-ssh-key.private_key))

          run:
            path: /bin/bash
            args:
              - -c
              - |
                #!/usr/bin/env bash

                set -eu

                scripts="${PWD}/bbr-pipeline-tasks-repo/scripts"

                export CLIENT_ID=
                # shellcheck disable=SC1090
                source "${scripts}/export-director-metadata"

                om_cmd curl -p /api/v0/deployed/products > deployed_products.json
                DEPLOYMENT_NAME=$(jq -r '.[] | select(.type == "p-Healthwatch-view") | .guid' "deployed_products.json")
                export DEPLOYMENT_NAME

                pushd rv-backup-artifact
                  ${scripts}/deployment-backup
                  tar -cvf rv-backup-$(date +%Y.%m.%d.%H.%M.%S).tar -- *
                popd

      - put: rv-backup-bucket
        params:
          file: rv-backup-artifact/rv-backup-*.tar
```

##  (Optional) Restoring Healthwatch from a BBR Backup
If you are running nightly backups, you have the ability to restore the Prometheus and Grafana data using Bosh Backup and Restore.
Be aware that this will delete any existing data on all Prometheus and Grafana VMs and restore them to when the backup was taken.

To restore, run the following commands:

```bash
mkdir -p $PATH_TO_DEPLOYMENT_BACKUP
tar xvf rv-backup-*.tar -C $PATH_TO_DEPLOYMENT_BACKUP
bbr deployment \
  --target $BOSH_TARGET \
  --username $BOSH_CLIENT \
  --password $BOSH_PASSWORD \
  --deployment $Healthwatch_VIEW_DEPLOYMENT_NAME \
  --ca-cert $PATH_TO_BOSH_SERVER_CERTIFICATE \
  restore \
  --artifact-path $PATH_TO_DEPLOYMENT_BACKUP
```
