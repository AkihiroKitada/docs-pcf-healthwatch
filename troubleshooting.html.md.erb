---
title: Troubleshooting Healthwatch
owner: Healthwatch
---

Healthwatch ships with the following dashboards
included under the **Healthwatch** folder in Grafana.
To access them, go to **Dashboard -> Manage -> Healthwatch** in Grafana.

## <a id="healthwatch-slos"></a> Healthwatch - SLOs
This dashboard will display a row
for each instance selected from the corresponding instance drop down
at the top of the page.
Each row has four panels:

  * The **Up** panel shows the current health of the [up](https://prometheus.io/docs/concepts/jobs_instances/) metric,
    where a `1` indicates it is healthy
    and a `0` or missing data indicates that it is not healthy.
  * The **Exporter SLO** panel shows the percent of time the exporter was up over the selected time range.
  * The **Error Budget Remaining** panel shows how much time, in minutes, are left,
    in order to stay within the selected Uptime SLO Target
    over the currently selected time range.
  * The **Minutes of Downtime** panel shows how much downtime, in minutes,
    the exporters have experienced during the selected time range.

## <a id="healthwatch-troubleshooting"></a> Healthwatch Troubleshooting

### <a id='viewing-the-prometheus-ui'></a> Viewing the Prometheus UI

The Prometheus UI allows users to see currently firing alerts
and the status of scrape targets, among other things.
This UI is not secure, however,
so we cannot make it available by default with the product.

In the event that you need to troubleshoot your Prometheus,
you can view the Prometheus UI by running the following command
and then navigating to `localhost:8080` in your browser.

```
bosh -d HEALTHWATCH-TILE-DEPLOYMENT-NAME ssh tsdb --opts='-L 8080:localhost:9090'
```

Where `HEALTHWATCH-TILE-DEPLOYMENT-NAME` is the name of the Healthwatch 2.x deployment
shown by the `bosh deployments` command.

<p class="note">
  <strong>Note</strong>:
  If the above command fails,
  look into the
  <a href="https://docs.pivotal.io/platform/2-10/customizing/trouble-advanced.html#prepare" target="_blank">
    Advanced Troubleshooting with the BOSH CLI
  </a>
  to learn how to configure BOSH SSH to run through a gateway.
</p>

### <a id='viewing-the-alertmanager-ui'></a> Viewing the Alertmanager UI

The Alertmanager UI allows users to see which alerts are currently firing
and gives the operator the option to silence them.
This UI is not secure, however,
so we cannot make it available by default with the product.

In the event that you need to troubleshoot your Alertmanager,
you can view the Alertmanager UI by running the following command
and then navigating to `localhost:8081` in your browser.

```
bosh -d HEALTHWATCH-TILE-DEPLOYMENT-NAME ssh tsdb --opts='-L 8082:localhost:10401'
```

Where `HEALTHWATCH-TILE-DEPLOYMENT-NAME` is the name of the Healthwatch 2.x deployment
shown by the `bosh deployments` command.

<p class="note">
  <strong>Note</strong>:
  If the above command fails,
  look into the
  <a href="https://docs.pivotal.io/platform/2-10/customizing/trouble-advanced.html#prepare" target="_blank">
    Advanced Troubleshooting with the BOSH CLI
  </a>
  to learn how to configure BOSH SSH to run through a gateway.
</p>

### <a id='grafana-smoke-test'></a> Grafana smoke test fails during Apply Changes

#### Problem

During apply changes, the Healthwatch smoke test fails with the following error message:
`querying for grafana up should be greater than 0`.

#### Cause

When Prometheus tries to scrape metrics from Grafana, it fails to scrape.
The potential causes include:

- Network issue between the `tsdb` vm instance and `grafana` vm instance
- Prometheus does not trust the certificate used by Grafana,
  due to it being a self-signed certificate used without the CA provided in the [Grafana Configuration](installing.html#grafana),
  or providing a CA that is not the one used to generate the certificate.

#### Troubleshooting

Follow the steps below to identify the issue.

1. `ssh` into the `tsdb` vm which hosts the Prometheus job
1. `curl http://localhost:9090/api/v1/targets | /var/vcap/packages/prometheus_backup_jq/bin/jq '.data.activeTargets[] | select(.scrapePool == "grafana")'`
    to obtain Grafana scrape target information, and the `lastError` field will show the reason why it fails to scrape.

### <a id='bosh-metrics-exporter'></a> BOSH Metrics Exporter fails to Connect to BOSH

#### Problem

```
ERROR [context.UaaContext [ForkJoinPool-1-worker-3]] javax.net.ssl.SSLHandshakeException: PKIX path validation failed: java.security.cert.CertPathValidatorException: Path does not chain with any of the trust anchors
ERROR [ingress.TokenCallCredentials [ForkJoinPool-1-worker-3]] Caught error retrieving UAA token: PKIX path validation failed: java.security.cert.CertPathValidatorException: Path does not chain with any of the trust anchors
INFO  [ingress.EventStreamObserver [ForkJoinPool-1-worker-3]] io.grpc.StatusRuntimeException: UNAUTHENTICATED
```

#### Cause

BOSH Metrics Exporter cannot verify the cert chain of BOSH Director UAA Server.

#### Troubleshooting

Follow the steps below to confirm the cert chain is invalid.

1. `ssh` into the `pks-exporter` vm which hosts the BOSH metrics exporter job
1. `openssl s_client -connect 10.0.0.5:8443` to obtain the certificate used by UAA,
    and save it to a file (`cert.pem`).
1. `openssl verify cert.pem` should return the literal string "OK",
    meaning the certificate is trusted and has a valid certificate chain.
    Otherwise, if any other message is returned, see
    [OpenSSL diagnostic documentation](https://www.openssl.org/docs/man1.0.2/man1/openssl-verify.html#DIAGNOSTICS)
    to troubleshoot and remedy.
    Certificate errors must be corrected before the BOSH Metrics Exporter will work.


## <a id="healthwatch-exporter-troubleshooting"></a> Healthwatch - Exporter Troubleshooting

This dashboard provides a detailed look at the performance of an individual exporter.
It is helpful to look at these dashboards
when you see spotty graphs for a particular metric type,
or if an exporter is not behaving as it should.
The dashboard consists of the following metrics:

  * Exporter Info: A listing of the `healthwatch_pasExporter_status` metric that contains
    runtime information for the exporter.
  * Exporter JVM Memory: A graph of `jvm_memory_bytes_used`, `jvm_memory_bytes_commited`, and
    `jvm_memory_bytes_init` over the time range,
    in order to check for memory leaks.
  * Ephemeral Disk Usage: A gauge of `system_disk_ephemeral_percent`, in order to check if the
    disk is nearing capacity.
  * Rate of Garbage Collection: A graph of the rate of `jvm_gc_collection_seconds_sum` over
    the time range,
    in order to check if JVM garbage collection is causing issues.
  * Rate of Envelope Ingress: A graph of the rate of `healthwatch_pasExporter_ingress_envelopes`
    over the time range,
    in order to see if there has been a spike in the amount
    of envelopes received by the exporter.
  * CPU Usage: A graph of `cpu_usage_user` over the time range,
    in order the see if the CPU used by the exporter is reaching capacity.
  * Exporter VM Threads: A graph of `jvm_threads_current` and `jvm_threads_peak` over the time range,
    in order to see if the exporter is leaking threads.
