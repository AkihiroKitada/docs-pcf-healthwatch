---
title: Healthwatch Release Notes
owner: Healthwatch
---

## <a id='2-1-0'></a> v2.1.0

**Release Date: March 18, 2021**

<div class="note breaking">
    <strong>Breaking Changes</strong>
    <ul>
      <li>Indicator Protocol is no longer shipped with Healthwatch.
      </li>
    </ul>

    <strong>Breaking Changes from 2.0.6 (Beta):</strong>
    <ul>
      <li>SSL validation will be checked by default in the Certificate Exporter.
      Added a configuration checkbox for skipping SSL validation for the Certificate Exporter in TAS/TKGI.
      Previously, Healthwatch would always skip SSL validation when checking the status of the Ops Manager Canary URL.
      This checkbox will need to be checked if Ops Manager has been configured with self-signed certificates.
      </li>
    </ul>
</div>

### Features

New features and changes in this release:

#### From 2.0.6

* Adds support for TKGI v1.10.

* Removes Indicator Protocol.

* Adds support for setting static IP addresses for the TSDB VMs.

#### From 1.8.x

* Uses Highly Available Alertmanager, Grafana, and Prometheus for alerting, display, and metric storage.

* Retains 42 days of all metric data (or 85 percent of persistent disk) in Prometheus, the popular time-series database.

* Has the ability to monitor multiple foundations from a single view.

* Monitors both VMware Tanzu Application Service and VMware Tanzu Kubernetes Grid Integrated Edition.

* Includes pre-provisioned dashboards for TAS v2.7, v2.8, v2.9, and v2.10
  as well as TKGI v1.8, v1.9, and v1.10.
  By default, Healthwatch 2.1 will dynamically set
  the dashboard version(s) based on the tile version(s) installed.

* Includes optional pre-provisioned dashboards for the Rabbit MQ and MySQL tiles.

* Includes a **Cert Expiration** dashboard displaying the
  remaining time until expiration for all discoverable certificates
  from the Ops Manager API as well as BOSH trusted certificates.

* Adds OAuth and LDAP as new authentication methods, in addition to UAA.

* Exposes composite metrics, or "Super Value Metrics",
  from Healthwatch v1.x to the metrics pipeline.
  This enables you to still see the same metrics from Healthwatch v1.x
  in your downstream log consumers if you upgrade from Healthwatch v1.x to Healthwatch v2.1.

* Adds a functional test and associated dashboard for the TKGI API.

* Separates Diego Capacity metrics by isolation segment in the **Diego/Capacity** dashboard.

* Exposes Ops Manager syslog configuration.

* All Jobs and Job Details dashboards ignore BOSH smoke test deployments.

### Resolved Issues

* PXC MySQL VMs disk could become completely full due to PXC storing binlogs in Healthwatch 2.0.x (Beta) releases.

### Known Issues

* <a href="#upgrade-from-om-23">Ops Manager pre v2.3 upgrade issue</a>.

* <a href="#pks-cluster-discovery-nsxt-ports">PKS Cluster Discovery needs manual configurations on NSX-T</a>.

### Packaged OSS Components

| Component    | Packaged Version |
|--------------|------------------|
| Prometheus   | 2.25.0           |
| Grafana      | 7.4.2            |
| Alertmanager | 0.21.0           |

## <a id="issues"></a> Known Issues

Healthwatch v2.1 includes the following known issues:

### <a name="upgrade-from-om-23"></a> Ops Manager pre v2.3 upgrade issue

When upgrading the Healthwatch v2.0 tile on a foundation that has been upgraded from
Ops Manager v2.3 or earlier, you may see the following error:

<pre class="terminal">
- Unable to render templates for job 'opsman-cert-expiration-exporter'. Errors are:
  - Error filling in template 'bpm.yml.erb' (line 9: Can't find property '["opsman_access_credentials.uaa_client_secret"]')
</pre>

This issue is resolved in Ops Manager v2.8.

To resolve this issue in Ops Manager v2.7 or earlier:

1. SSH into your Ops Manager VM.

1. Change the user to `root`.

1. Open the Rails console by running:

    ```
    > cd /home/tempest-web/tempest/web; RAILS_ENV='production' TEMPEST_INFRASTRUCTURE='DEPLOYMENT-IAAS' TEMPEST_WEB_DIR='/home/tempest-web' SECRET_KEY_BASE='1234' DATA_ROOT='/var/tempest' LOG_DIR='/var/log/opsmanager' su tempest-web --command 'bundle exec rails console'
    ```
    Where `DEPLOYMENT-IAAS` is either `google`, `aws`, `azure`, `vsphere`, or `openstack`, depending on the IaaS of your Ops Manager deployment.

1. Set the decryption passphrase by running:

    ```
    irb(main):001:0> EncryptionKey.instance.passphrase = 'DECRYPTION-PASSPHRASE'
    ```
    Where `DECRYPTION-PASSPHRASE` is the decryption passphrase you want to set.

1. Update the UAA restricted view access client secret by running:

    ```
    irb(main):001:0> Uaa::UaaConfig.instance.update_attributes(restricted_view_api_access_client_secret: SecureRandom.hex)
    ```

1. Exit the Rails console and restart the `tempest-web` service by running:

    ```
    irb(main):001:0> exit
    > service tempest-web restart
    ```

### <a name="pks-cluster-discovery-nsxt-ports"></a> PKS Cluster Discovery needs manual configurations on NSX-T.

When PKS Cluster Discovery is used with PKS deployed on vSphere NSX-T, the load balancer created by NSX-T
does not have all the ports open that are required for PKS Cluster Discovery to scrape from on-demand PKS clusters correctly.
One workaround is to manually modify the load balancer through the NSX-T Manager API to open the
`10200`, `10251`, `10252`, and `8443` ports.

To open the ports:

1. Fetch the list of virtual servers by running:

    ```
    curl -u 'NSX-T-USERNAME:NSX-T-PASSWORD' \
      "https://NSX-MGR-IP-OR-FQDN/api/v1/loadbalancer/virtual-servers" | jq .
    ```
    Where:
    - `NSX-T-USERNAME` is the username you use to log in to the NSX-T console.
    - `NSX-T-PASSWORD` is the password you use to log in to the NSX-T console.
    - `NSX-MGR-IP-OR-FQDN` is the IP address or fully-qualified domain name (FQDN) of your NSX-T console.

1. In the output, look for the JSON array item that has the `display_name` starting with `lb-pks`
       and ending with `virtual-server` and copy the `id` field.

1. Fetch the current configuration for the load balancer by running:

    ```
    curl -u 'NSX-T-USERNAME:NSX-T-PASSWORD' \
      https://NSX-MGR-IP-OR-FQDN/api/v1/loadbalancer/virtual-servers/VIRTUAL-SERVER-UUID
    ```
    Where:
    - `NSX-T-USERNAME` is the username you use to log in to the NSX-T console.
    - `NSX-T-PASSWORD` is the password you use to log in to the NSX-T console.
    - `NSX-MGR-IP-OR-FQDN` is the IP address or fully-qualified domain name (FQDN) of your NSX-T console.
    - `VIRTUAL-SERVER-UUID` is the unique ID that identifies the load balancer.

1. Save the JSON that is returned by this command to a file.

1. Modify the JSON file to include the additional ports:

    ```
    {
        "...": "...",
        "ports": [
            "8443",
            "10200",
            "10251",
            "10252"
        ],
        "...": "..."
    }
    ```

1. Send a PUT request to the API to update the virtual server by running:

    ```
    curl -X PUT -u 'NSX-T-USERNAME:NSX-T-PASSWORD' \
      https://NSX-MGR-IP-OR-FQDN/api/v1/loadbalancer/virtual-servers/VIRTUAL-SERVER-UUID \
      -H 'X-Allow-Overwrite: true' -H 'Content-type: Application/json' \
      -d 'MODIFIED-JSON-DATA'
    ```
    Where:
    - `NSX-T-USERNAME` is the username you use to log in to the NSX-T console.
    - `NSX-T-PASSWORD` is the password you use to log in to the NSX-T console.
    - `NSX-MGR-IP-OR-FQDN` is the IP address or fully-qualified domain name (FQDN) of your NSX-T console.
    - `VIRTUAL-SERVER-UUID` is the unique ID that identifies the load balancer.
    - `MODIFIED-JSON-DATA` contains the additional ports you want to add.
    
### <a name="kubernetes-node-panels-no-data"></a> No Data on TKGI Kubernetes Node Panels

This is due to un-exported metrics in Kubernetes.
See <a href="https://github.com/kubernetes/kubernetes/pull/97006">Fix missing cadvisor machine metrics</a>
for more information.
Upgrade to a TKGI version that has a version of Kubernetes greater than 1.19.6 to resolve this issue.
